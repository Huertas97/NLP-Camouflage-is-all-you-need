{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077f781-c0f6-46e9-b97c-bfcfced6b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! This way you can navigate through $home an get the path to the NAS data\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "home = str(Path.home())\n",
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc14990c-bbd4-4abd-b219-634cbdfa2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 08:11:00.368829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-04 08:11:00.654352: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-04 08:11:00.710962: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-04 08:11:01.753536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-04 08:11:01.753644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-04 08:11:01.753657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy version: 3.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alvaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alvaro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy, copy\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from codetiming import Timer\n",
    "import pyleetspeak\n",
    "# print(f\"Pyleetspeak version: {pyleetspeak.__version__}\")\n",
    "print(f\"Spacy version: {spacy.__version__}\")\n",
    "\n",
    "\n",
    "from pyleetspeak.pyleetspeak import WordCamouflage_Augmenter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e9d1f-92aa-4420-b0b1-e7f5dca9cd3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fd258e-8ab0-4b8f-94b9-944e22531ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(data_tuples, nlp, labels):\n",
    "    \"\"\"_summary_\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for text, label in tqdm(nlp.pipe(data_tuples, as_tuples=True), total = len(data_tuples), desc = \"Making docs\"):\n",
    "        doc = nlp(text)\n",
    "\n",
    "        for l in labels:\n",
    "            # Hay que hacer todos los labels\n",
    "            doc.cats[l] = label == l   \n",
    "\n",
    "        # put them into a nice list\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def leet_data(text, generator):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    NER_data, ori_data = generator.generate_data(\n",
    "            sentence=text\n",
    "            # important_kws = [r\"\\bpfizer\\b\", r\"control\\b\", r\"vacuna\\b\", r\"vaccines\\b\"],\n",
    "        )\n",
    "\n",
    "    leet_text, _ =  NER_data[0]\n",
    "    return leet_text\n",
    "\n",
    "\n",
    "# function to create dataframes witrh a percentage leet tweets\n",
    "def create_leet_df(df_ori, frac, generator, column_to_leet):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        frac (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "        column_to_leet (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_leeted = copy(df_ori)\n",
    "    df_leeted[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract fraction to leet\n",
    "    df_to_leet = df_leeted.sample(frac=frac, random_state=42)\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_leet[column_to_leet] = df_to_leet[column_to_leet].progress_apply(leet_data,  generator=generator_EN)\n",
    "    df_to_leet[\"Camouflaged\"] = True\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(f\"Nan values in '{column_to_leet}' column: \", df_to_leet[column_to_leet].isna().sum())\n",
    "\n",
    "    # Substitute the original rows by the leeted version\n",
    "    cols = list(df_leeted.columns) \n",
    "    df_leeted.loc[df_leeted.index.isin(df_to_leet.index), cols] = df_to_leet[cols]    \n",
    "\n",
    "    display( df_leeted.groupby(\"Camouflaged\").count() ) \n",
    "    \n",
    "    return df_leeted\n",
    "\n",
    "\n",
    "def create_augmented_df(df_ori, frac, generator):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df_ori (_type_): _description_\n",
    "        frac (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_augmented = copy(df_ori)\n",
    "    df_augmented[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract different fraction to leet \n",
    "    df_to_augment = df_augmented.sample(frac=frac, random_state=42)\n",
    "    df_to_augment_2 = df_augmented.sample(frac=frac, random_state=4)\n",
    "    df_to_augment_3 = df_augmented.sample(frac=frac, random_state=12)\n",
    "\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_augment[\"tweet\"] = df_to_augment[\"tweet\"].progress_apply(leet_data,  generator=generator)\n",
    "    df_to_augment[\"Camouflaged\"] = True\n",
    "\n",
    "    df_to_augment_2[\"tweet\"] = df_to_augment_2[\"tweet\"].progress_apply(leet_data,  generator=generator)\n",
    "    df_to_augment_2[\"Camouflaged\"] = True\n",
    "\n",
    "    df_to_augment_3[\"tweet\"] = df_to_augment_3[\"tweet\"].progress_apply(leet_data,  generator=generator)\n",
    "    df_to_augment_3[\"Camouflaged\"] = True\n",
    "\n",
    "\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(\"Nan values in df_train_offen_to_leet['tweet'] column: \", df_to_augment[\"tweet\"].isna().sum())\n",
    "    print(\"Nan values in df_train_offen_to_leet['tweet'] column: \", df_to_augment_2[\"tweet\"].isna().sum())\n",
    "    print(\"Nan values in df_train_offen_to_leet['tweet'] column: \", df_to_augment_3[\"tweet\"].isna().sum())\n",
    "\n",
    "    # Add the augmented tweets to the original dataframe\n",
    "    df_augmented = pd.concat([df_augmented, df_to_augment, df_to_augment_2, df_to_augment_3], ignore_index=True)\n",
    "    # suffle the dataframe\n",
    "    df_augmented = df_augmented.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "    display( df_augmented.groupby(\"Camouflaged\").count() ) \n",
    "    \n",
    "    return df_augmented\n",
    "\n",
    "\n",
    "\n",
    "# Create a function to save pandas dataframe to spacy binary file\n",
    "def pd_2_spacy(df_train, df_dev, df_test, train_output_path, dev_output_path, test_output_path, labels, lang=\"en\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df_train (_type_): _description_\n",
    "        df_dev (_type_): _description_\n",
    "        df_test (_type_): _description_\n",
    "        train_output_path (_type_): _description_\n",
    "        dev_output_path (_type_): _description_\n",
    "        test_output_path (_type_): _description_\n",
    "        lang (str, optional): _description_. Defaults to \"en\".\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Spact empty model\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    if df_train is not None:\n",
    "        # tuple of tuples. Each nested tuple is (Tweet, Label)\n",
    "        train_data_tuples = tuple(df_train.iloc[:, [0,1]].itertuples(index=False, name=None))\n",
    "                \n",
    "        # Make spacy DocBin\n",
    "        train_docs = make_docs(train_data_tuples, nlp, labels)\n",
    "\n",
    "        # Make outpath directory with Pathlib\n",
    "        Path(train_output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # save to binary file \n",
    "        train_doc_bin = DocBin(docs=train_docs)\n",
    "        train_doc_bin.to_disk(train_output_path)\n",
    "        print(f\"Processed Train {len(train_data_tuples)} documents: {train_output_path}\")        \n",
    "\n",
    "    if df_dev is not None:\n",
    "        dev_data_tuples = tuple(df_dev.iloc[:, [0,1]].itertuples(index=False, name=None))\n",
    "        dev_docs = make_docs(dev_data_tuples, nlp, labels)\n",
    "        Path(dev_output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        dev_doc_bin = DocBin(docs=dev_docs)\n",
    "        dev_doc_bin.to_disk(dev_output_path)\n",
    "        print(f\"Processed Dev {len(dev_data_tuples)} documents: {dev_output_path}\")\n",
    "\n",
    "    if df_test is not None:\n",
    "        test_data_tuples = tuple(df_test.iloc[:, [0,1]].itertuples(index=False, name=None))\n",
    "        test_docs = make_docs(test_data_tuples, nlp, labels)\n",
    "        Path(test_output_path).parent.mkdir(parents=True, exist_ok=True)   \n",
    "\n",
    "        test_doc_bin = DocBin(docs=test_docs)\n",
    "        test_doc_bin.to_disk(test_output_path)\n",
    "        print(f\"Processed Test {len(test_data_tuples)} documents: {test_output_path}\")    \n",
    "\n",
    "\n",
    "def leet_data_augmenter(text, augmenter):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    leet_text = augmenter.transform(\n",
    "            text\n",
    "        )\n",
    "    return leet_text\n",
    "\n",
    "\n",
    "# function to create dataframes witrh a percentage leet tweets\n",
    "def create_leet_augmenter_df(df_ori, frac, augmenter, column_to_leet):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        frac (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "        column_to_leet (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_leeted = copy(df_ori)\n",
    "    df_leeted[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract fraction to leet\n",
    "    df_to_leet = df_leeted.sample(frac=frac, random_state=42)\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_leet[column_to_leet] = df_to_leet[column_to_leet].progress_apply(leet_data_augmenter,  augmenter=augmenter)\n",
    "    df_to_leet[\"Camouflaged\"] = True\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(f\"Nan values in '{column_to_leet}' column: \", df_to_leet[column_to_leet].isna().sum())\n",
    "\n",
    "    # Substitute the original rows by the leeted version\n",
    "    cols = list(df_leeted.columns) \n",
    "    df_leeted.loc[df_leeted.index.isin(df_to_leet.index), cols] = df_to_leet[cols]    \n",
    "\n",
    "    display( df_leeted.groupby(\"Camouflaged\").count() ) \n",
    "    \n",
    "    return df_leeted\n",
    "\n",
    "\n",
    "\n",
    "# Define la funci√≥n random_augmenter\n",
    "def random_augly_augmenter(input_text):\n",
    "    # define augmenters\n",
    "    aug_ReplaceSimilar = textaugs.ReplaceSimilarChars(p=0.4)\n",
    "    aug_ReplaceSimUnicode = textaugs.ReplaceSimilarUnicodeChars(p=0.4)\n",
    "    aug_InsertPunctuation = textaugs.InsertPunctuationChars(granularity=\"all\", cadence=4, p=0.4)\n",
    "\n",
    "    aug_ReplaceSim_Punct = textaugs.Compose(\n",
    "        transforms=[\n",
    "            textaugs.ReplaceSimilarChars(p=0.4),\n",
    "            textaugs.InsertPunctuationChars(granularity=\"all\", cadence=4, p=1)\n",
    "        ],\n",
    "        p=0.4\n",
    "    )\n",
    "\n",
    "    aug_ReplaceSimUnicode_Punct = textaugs.Compose(\n",
    "        transforms=[\n",
    "            textaugs.ReplaceSimilarUnicodeChars(\n",
    "                p=1,\n",
    "            ),\n",
    "            textaugs.InsertPunctuationChars(granularity=\"all\", cadence=4, p=0.4)\n",
    "        ],\n",
    "        p=1,\n",
    "    )\n",
    "\n",
    "    augmenters = [\n",
    "        aug_ReplaceSimilar,\n",
    "        aug_ReplaceSimUnicode,\n",
    "        aug_InsertPunctuation,\n",
    "        aug_ReplaceSim_Punct,\n",
    "        aug_ReplaceSimUnicode_Punct,\n",
    "    ]\n",
    "\n",
    "    random_augmenter = rng.choice(\n",
    "        augmenters, replace=True, \n",
    "        # p=[0.5, 0.2, 0.12, 0.12, 0.06]\n",
    "    )\n",
    "    leet_text = random_augmenter(input_text)\n",
    "    \n",
    "    if isinstance(leet_text, list):\n",
    "        # Convierte la lista en una cadena usando un separador (por ejemplo, un espacio)\n",
    "        leet_text = \" \".join(leet_text)\n",
    "    \n",
    "    return leet_text\n",
    "\n",
    "def create_augly_augmentation(df_ori, frac, augmenter, column_to_leet):\n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_leeted = copy(df_ori)\n",
    "    df_leeted[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract fraction to leet\n",
    "    df_to_leet = df_leeted.sample(frac=frac, random_state=42)\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_leet[column_to_leet] = df_to_leet[column_to_leet].progress_apply(augmenter)\n",
    "    df_to_leet[\"Camouflaged\"] = True\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(f\"Nan values in '{column_to_leet}' column: \", df_to_leet[column_to_leet].isna().sum())\n",
    "\n",
    "    # Substitute the original rows by the leeted version\n",
    "    cols = list(df_leeted.columns) \n",
    "    df_leeted.loc[df_leeted.index.isin(df_to_leet.index), cols] = df_to_leet[cols]    \n",
    "\n",
    "    display( df_leeted.groupby(\"Camouflaged\").count() )\n",
    "    \n",
    "    return df_leeted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4491a90-cc40-482a-952c-69b57ccdb2ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [OffenSenEval 2019](https://competitions.codalab.org/competitions/20011)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f271d-3038-474a-8fa2-b25654f250dc",
   "metadata": {},
   "source": [
    "Sub-task A: Offensive or not\n",
    "\n",
    "In this sub-task we are interested in the identification of offensive posts and posts containing any form of (untargeted) profanity. In this sub-task there are **2 categories** in which the tweet could be classified -\n",
    "\n",
    "- **Not Offensive** - This post does not contain offense or profanity.  Non-offensive posts do not include any form of offense or profanity.\n",
    "\n",
    "- **Offensive** - This post contains offensive language or a targeted (veiled or direct) offense.  In our annotation, we label a post as offensive if it  contains any form of non-acceptable language (profanity) or a targeted offense which can be veiled or direct. To sum up this category includes insults, threats, and posts containing profane language and swear words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b0e4e-3324-413e-90a5-afc37bb8ee14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Pandas data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af362b0",
   "metadata": {},
   "source": [
    "Load the orignal non-camouflage data and transform it to spacy data format for Naive models training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff19dda-5a72-4b00-af55-2200da7c747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtask_a</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT</th>\n",
       "      <td>7933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF</th>\n",
       "      <td>3953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet\n",
       "subtask_a       \n",
       "NOT         7933\n",
       "OFF         3953"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtask_a</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT</th>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF</th>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet\n",
       "subtask_a       \n",
       "NOT          882\n",
       "OFF          439"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT</th>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet\n",
       "test_label       \n",
       "NOT           620\n",
       "OFF           240"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Temporal Train\n",
    "# train_offen_path = Path(home).joinpath(\"data/WordCamouflage_Resiliance/Datasets/OffensEval_2019/OffensEval_2019/OLIDv1.0/olid-training-v1.0.tsv\")\n",
    "train_offen_path = Path(home).joinpath(\"work/Code/WordCamouflage_Resiliance/code/tmp_data/Offen/olid-training-v1.0.tsv\")\n",
    "\n",
    "df_train_tmp_offen = pd.read_csv( train_offen_path, sep = \"\\t\").drop(labels = [\"id\", \"subtask_b\", \"subtask_c\"], axis = 1)\n",
    "\n",
    "# drop duplicates\n",
    "df_train_tmp_offen = df_train_tmp_offen.drop_duplicates(subset = [\"tweet\"])\n",
    "\n",
    "labels_offen = list(df_train_tmp_offen[\"subtask_a\"].unique())\n",
    "\n",
    "# Split into Train and Dev\n",
    "df_train_offen, df_dev_offen = train_test_split(\n",
    "    df_train_tmp_offen,\n",
    "    random_state=42,\n",
    "    test_size=0.1,\n",
    "    stratify=df_train_tmp_offen[\"subtask_a\"].to_numpy(),\n",
    ")\n",
    "\n",
    "\n",
    "# Load Test Data and Label\n",
    "test_data_offen_path = Path(home).joinpath(\"work/Code/WordCamouflage_Resiliance/code/tmp_data/Offen/testset-levela.tsv\")\n",
    "test_label_offen_path = Path(home).joinpath(\"work/Code/WordCamouflage_Resiliance/code/tmp_data/Offen/labels-levela.csv\")\n",
    "\n",
    "df_test_data_offen = pd.read_csv( test_data_offen_path, sep = \"\\t\")\n",
    "df_test_label_offen = pd.read_csv( test_label_offen_path, sep = \",\", header=None, names=[\"id\", \"test_label\"])\n",
    "\n",
    "# Merge Test Data and Test Label\n",
    "df_test_offen = pd.merge(df_test_data_offen, df_test_label_offen, on=\"id\", how=\"outer\").drop(labels = [\"id\"], axis = 1)\n",
    "\n",
    "print(\"Train\")\n",
    "display(df_train_offen.groupby(\"subtask_a\").count())\n",
    "\n",
    "print(\"Dev\")\n",
    "display(df_dev_offen.groupby(\"subtask_a\").count())\n",
    "\n",
    "print(\"Test\")\n",
    "display(df_test_offen.groupby(\"test_label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f0915",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### From Pandas to Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff73b15-1670-48fe-86eb-2af93514dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train spacy, Dev spacy, Test spacy\n",
    "train_offen_output_path = \"./train_offen_toy.spacy\"\n",
    "dev_offen_output_path = \"./dev_offen_toy.spacy\"\n",
    "test_offen_output_path = \"./test_offen_toy.spacy\"\n",
    "\n",
    "# tuple of tuples. Each nested tuple is (Tweet, Label)\n",
    "train_offen_data_tuples = tuple(df_train_offen.itertuples(index=False, name=None))\n",
    "dev_offen_data_tuples = tuple(df_dev_offen.itertuples(index=False, name=None))\n",
    "test_offen_data_tuples = tuple(df_test_offen.itertuples(index=False, name=None))\n",
    "\n",
    "# Spact empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Make spacy DocBin\n",
    "train_offen_docs = make_docs(train_offen_data_tuples, nlp)\n",
    "dev_offen_docs = make_docs(dev_offen_data_tuples, nlp)\n",
    "test_offen_docs = make_docs(test_offen_data_tuples, nlp)\n",
    "\n",
    "# save to binary file \n",
    "train_offen_doc_bin = DocBin(docs=train_offen_docs)\n",
    "train_offen_doc_bin.to_disk(train_offen_output_path)\n",
    "print(f\"Processed Train {len(train_offen_data_tuples)} documents: {train_offen_output_path}\")\n",
    "\n",
    "dev_offen_doc_bin = DocBin(docs=dev_offen_docs)\n",
    "dev_offen_doc_bin.to_disk(dev_offen_output_path)\n",
    "print(f\"Processed Dev {len(dev_offen_data_tuples)} documents: {dev_offen_output_path}\")\n",
    "\n",
    "test_offen_doc_bin = DocBin(docs=test_offen_docs)\n",
    "test_offen_doc_bin.to_disk(test_offen_output_path)\n",
    "print(f\"Processed Test {len(test_offen_data_tuples)} documents: {test_offen_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7fc83",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Camouflage Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef181b",
   "metadata": {},
   "source": [
    "In the following cells we generate the data for model fitting with the _static_ strategy for the different levels of complecity, word camouflage ratio and instance camouflage ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdd862-e534-4090-b58c-4043f6bfe979",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Level 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213d5ac-2b8e-4bf6-9606-fec7120e2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_1.1\"\n",
    "resiliance_easy = [\"basic_leetspeak\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.8,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.5,\n",
    "        method=resiliance_easy,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ccfe5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Level 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a4ca7-2815-43be-8532-fbeb12ccfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_1.2\"\n",
    "resiliance_easy = [\"basic_leetspeak\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.8,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.5,\n",
    "        method=resiliance_easy,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278afdcd-9957-4b9d-99cb-1af0e5d66dda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576af59-96f9-45f0-805b-989af37ff4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_2.1\"\n",
    "resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        method=resiliance_intermediate,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfebf3f7-bfe4-4a3a-9480-9339591db058",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a3e97-a229-4f81-91e5-836b8ed8a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_2.2\"\n",
    "resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        method=resiliance_intermediate,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7a22d-ceeb-4f7c-bc89-506de74e1923",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a822c-4206-482c-8438-15ae34a85603",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_3.1\"\n",
    "resiliance_advanced = [\"advanced_leetspeak\", \"punct_camo\", \"inv_camo\"]\n",
    "\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5,\n",
    "        method=resiliance_advanced,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6124149",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Level 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d99c38-2493-4953-bd17-e090dac1c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_3.2\"\n",
    "resiliance_advanced = [\"advanced_leetspeak\", \"punct_camo\", \"inv_camo\"]\n",
    "\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5,\n",
    "        method=resiliance_advanced,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41523f78-9815-4cb2-9a90-ca14fef89e57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level Mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6bdef",
   "metadata": {},
   "source": [
    "Mixed camouflaged version data combining the different levels of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb0bfa-917f-4b9b-8acf-3e129280498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_Mixed\"\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447877",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cc146-c051-4dfe-b64a-18b6dc86d43c",
   "metadata": {},
   "source": [
    "#### Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d9e17-c7b4-4342-b887-77da75d94a5a",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m spacy train  [config-path] --output [output-path] --gpu-id X --code [augmenter-script]\n",
    "```\n",
    "\n",
    "For example, training the BERT encoder-model in dynamic strategy with 10% of data instances camouflaged with mixed camouflaged. \n",
    "```bash\n",
    "python -m spacy train ~work/NLP-Camouflage-is-all-you-need/Experiments/Fine_tuning_robustness/configs/offen_semeval_10_dynamic_config.cfg --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-dynamic --gpu-id 0 --code functions.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66216dc-ea66-4522-b8f0-213c74b90875",
   "metadata": {},
   "source": [
    "#### Static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f68c2-b97f-4abb-bb0e-7521f56b6caf",
   "metadata": {},
   "source": [
    "In this case, the `config-path` track a config file where the data used is the one previously camouflaged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286e965-6a03-41f8-82d2-01a014cbfc83",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m spacy train  [config-path] --output [output-path] --gpu-id X \n",
    "```\n",
    "\n",
    "For example, training the BERT encoder-model in static strategy with 10% of data instances camouflaged with mixed camouflaged. \n",
    "```bash\n",
    "python -m spacy train ~work/NLP-Camouflage-is-all-you-need/Experiments/Fine_tuning_robustness/configs/offen_semeval_10_static_config.cfg --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-static --gpu-id 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be746e7f-744b-4fa5-9a41-1ae375a53cde",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20881359-3fe2-4258-8e4e-fa2e24bc00ab",
   "metadata": {},
   "source": [
    "#### Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580e40e-a8fc-4019-aef3-a198ddc65fa1",
   "metadata": {},
   "source": [
    "10%, Level 1 v2\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate [model-to-path] [test-data-path] --gpu-id X --output [output-results-path]\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-dynamic ~work/NLP-Camouflage-is-all-you-need/Spacy_Data/Offen_SemEval_2019/Leet_Data/Level_1.2/10_per/test.spacy --gpu-id 1 --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-dynamic/model-best/test_10_level_1.2_result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f017906-8fe1-4633-835d-73f3226f7245",
   "metadata": {},
   "source": [
    "#### Static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a748c7-5675-423b-adc5-4ba033923d06",
   "metadata": {},
   "source": [
    "10%, Level 1 v2\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate [model-to-path] [test-data-path] --gpu-id X --output [output-results-path]\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-static ~work/NLP-Camouflage-is-all-you-need/Spacy_Data/Offen_SemEval_2019/Leet_Data/Level_1.2/10_per/test.spacy --gpu-id 1 --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-static/model-best/test_10_level_1.2_result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0d455-8957-4b2c-b6a5-78288795ea07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AugLy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a29dd",
   "metadata": {},
   "source": [
    "External validation with AugLy augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d53474a-04a1-4d65-992d-f8f16a298bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import augly.text as textaugs\n",
    "import numpy as np\n",
    "\n",
    "# Crea un generador con la semilla 42\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08be7e2-e9c6-45ce-8985-45ed3321a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1189/1189 [00:01<00:00, 853.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>10697</td>\n",
       "      <td>10697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False        10697      10697\n",
       "True          1189       1189"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 132/132 [00:00<00:00, 895.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         1189       1189\n",
       "True           132        132"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 86/86 [00:00<00:00, 755.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          774         774\n",
       "True            86          86"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:04<00:00, 2919.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 3887.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 2510.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2972/2972 [00:04<00:00, 726.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>8914</td>\n",
       "      <td>8914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2972</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         8914       8914\n",
       "True          2972       2972"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 330/330 [00:00<00:00, 786.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>991</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>330</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False          991        991\n",
       "True           330        330"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 215/215 [00:00<00:00, 823.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>645</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          645         645\n",
       "True           215         215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:04<00:00, 2454.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/25_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 3280.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/25_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 2185.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/25_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5943/5943 [00:07<00:00, 820.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5943</td>\n",
       "      <td>5943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>5943</td>\n",
       "      <td>5943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         5943       5943\n",
       "True          5943       5943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 660/660 [00:00<00:00, 820.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>661</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>660</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False          661        661\n",
       "True           660        660"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 430/430 [00:00<00:00, 842.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          430         430\n",
       "True           430         430"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:05<00:00, 2042.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/50_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 2490.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/50_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 1706.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/50_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 8914/8914 [00:10<00:00, 827.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2972</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>8914</td>\n",
       "      <td>8914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         2972       2972\n",
       "True          8914       8914"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 991/991 [00:01<00:00, 860.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>330</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>991</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False          330        330\n",
       "True           991        991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 645/645 [00:00<00:00, 813.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>645</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          215         215\n",
       "True           645         645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:07<00:00, 1629.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/75_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 1953.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/75_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 1374.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/75_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 11886/11886 [00:14<00:00, 839.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>11886</td>\n",
       "      <td>11886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "True         11886      11886"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1321/1321 [00:01<00:00, 852.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "True          1321       1321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 860/860 [00:01<00:00, 813.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>860</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True           860         860"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:08<00:00, 1370.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/100_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 1744.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/100_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 1213.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/100_per/test.spacy\n"
     ]
    }
   ],
   "source": [
    "resiliance_level = \"AugLy\"\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13cbe93",
   "metadata": {},
   "source": [
    "##### Evaluation of the models on AugLy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf034d-93e4-457d-8782-df9d0e6d06d2",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m spacy evaluate [path-to-model] [path-to-augly-data] --gpu-id X --output [output-path]\n",
    "```\n",
    "\n",
    "For example for BERT (encoder-only model):\n",
    "```bash\n",
    "python -m spacy evaluate ~/work/Code/WordCamouflage_Resiliance/code/output_models/Offen_SemEval_2019/bert-base-uncased_naive/model-best ~/work/Code/WordCamouflage_Resiliance/code/Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/test.spacy --gpu-id 0 --output ~/work/Code/WordCamouflage_Resiliance/code/output_models/Offen_SemEval_2019/bert-base-uncased_naive/model-best/test_augly_10_result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9fad396-0daf-4ff4-aeee-d256f72b01ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet         #ChristineBlaseyFord is your #Kavanaugh accuser... #Liberals try this EVERY time... #ConfirmJudgeKavanaugh URL\n",
       "test_label                                                                                                               OFF\n",
       "Name: 30, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_offen.iloc[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "706bad69-31b3-4da8-ae34-10fce6d5350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_ReplaceSimilar = textaugs.ReplaceSimilarChars(p=1)\n",
    "aug_InsertPunctuation = textaugs.InsertPunctuationChars(granularity=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d2cc260-d3fe-403d-9624-b65649700ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H!E!l!l!o!o w.o.r.l.d']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_InsertPunctuation(\"HElloo world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78ac36ed-9ff2-4f15-92e1-230f81574364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>#SilsilaBadallteRishtonKa „ÉÜag is filled with appreciation o‚Ç£ @ USER only.... Never watch¬£ƒê this serial but I always want‚í∫d appreciation for Adi≈•i... She i≈ú underrated s·é•nce sta‚Ñü‚ì£ing of the sho≈µ... She √ès w·ª¢rth ·ªöf aƒæl this app·ñáeciatƒ¨o≈ã.. üëè üëè üëè</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>#ChristineBlaseyFord is your#K@vanau9h accuser...#L7|3erals t|2y +his EVERY tim3...#ConfirmJudgeKavanaugh URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>@ USER How is she hiding her ugly perso^ali+y. S|-|e is t|-|e worst.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>@USE:R om:g is: he :for :real: ?!?:!!??:????:) th:is h:appe:ned :in p:eru :like: 40 :year:s ag:o an:d th:e In:ti d:eval:uate:d so: fuc:king: muc:h th:at t:hey :had :to c:omol:etel:y ch:ange: the: coi:n sy:stem: bec:ause: our: mon:ey w:as w:orth:less: i-</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>#Halloween Sign If You Are ReŒ¨·ö¶ing Thi‚Ç∞#CreepingUp Behind ‡Ø∂ou ·èáood Wall ·ó´eƒã√¥ration#tm≈§inst√£#cwsigns URL via @ USER</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>#Antifa invaded a ·å†emoria»¥ in recognition of Katƒô Steinle in Au≈ûtin, Texas, a‚Ç¶d tried ·π´o comfort hi·ó∞ it lo·ªûked like when it w√Äs struggling, but ‚Ñças ‚Ñùeturn√äd to the Southern District √ìf N√âw York, is r‚àëcu≈ùed fro·õñ t‚Ñèe Micha“ø≈Ç Cohen raid, Trump has my full support. Moving on.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>I'am s√î sleepy.. {Bo„ÉÜ c√ºddle¬ß up to yo‡∏ô} {#Rogu·∫æBo≈ß} \"</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>1) WoVV, saf3 sex! 7hat's hot! We lov3 a butch top who @lso receives! URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>#MeetT‚Ñçe≈öpea‚ìÄe≈ós üôå @ U’èER will ‚ìürese”át in œåur event OI‡∏ü 2018: Finpact-Global Impact through Fin√Ö‚ìùcial Technologies. She is Senior Advisor Gro≈≤p Sustainable ·∏ûina≈áce and worked on green e‚Ç¶ergy ·∫¥nd ƒàlimat√© ri$k. Join us to m“æet Thina URL#oiw2018 URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>#WednesdayWisdom A^tifa calls +he right fascist when, in 4ll reality, th3y and the left are following the sa|V|e s&lt;enari() as the Thi12d Reich: indoctrination of []ur youth, tryi^g to control minorities and a total lack of understanding or kn[]wled6e of hi$toI2y.#W4lkAway</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                tweet  \\\n",
       "23                                 #SilsilaBadallteRishtonKa „ÉÜag is filled with appreciation o‚Ç£ @ USER only.... Never watch¬£ƒê this serial but I always want‚í∫d appreciation for Adi≈•i... She i≈ú underrated s·é•nce sta‚Ñü‚ì£ing of the sho≈µ... She √ès w·ª¢rth ·ªöf aƒæl this app·ñáeciatƒ¨o≈ã.. üëè üëè üëè   \n",
       "30                                                                                                                                                                      #ChristineBlaseyFord is your#K@vanau9h accuser...#L7|3erals t|2y +his EVERY tim3...#ConfirmJudgeKavanaugh URL   \n",
       "39                                                                                                                                                                                                               @ USER How is she hiding her ugly perso^ali+y. S|-|e is t|-|e worst.   \n",
       "63                      @USE:R om:g is: he :for :real: ?!?:!!??:????:) th:is h:appe:ned :in p:eru :like: 40 :year:s ag:o an:d th:e In:ti d:eval:uate:d so: fuc:king: muc:h th:at t:hey :had :to c:omol:etel:y ch:ange: the: coi:n sy:stem: bec:ause: our: mon:ey w:as w:orth:less: i-   \n",
       "65                                                                                                                                                                 #Halloween Sign If You Are ReŒ¨·ö¶ing Thi‚Ç∞#CreepingUp Behind ‡Ø∂ou ·èáood Wall ·ó´eƒã√¥ration#tm≈§inst√£#cwsigns URL via @ USER   \n",
       "..                                                                                                                                                                                                                                                                                ...   \n",
       "836  #Antifa invaded a ·å†emoria»¥ in recognition of Katƒô Steinle in Au≈ûtin, Texas, a‚Ç¶d tried ·π´o comfort hi·ó∞ it lo·ªûked like when it w√Äs struggling, but ‚Ñças ‚Ñùeturn√äd to the Southern District √ìf N√âw York, is r‚àëcu≈ùed fro·õñ t‚Ñèe Micha“ø≈Ç Cohen raid, Trump has my full support. Moving on.   \n",
       "846                                                                                                                                                                                                                            I'am s√î sleepy.. {Bo„ÉÜ c√ºddle¬ß up to yo‡∏ô} {#Rogu·∫æBo≈ß} \"   \n",
       "850                                                                                                                                                                                                         1) WoVV, saf3 sex! 7hat's hot! We lov3 a butch top who @lso receives! URL   \n",
       "856                            #MeetT‚Ñçe≈öpea‚ìÄe≈ós üôå @ U’èER will ‚ìürese”át in œåur event OI‡∏ü 2018: Finpact-Global Impact through Fin√Ö‚ìùcial Technologies. She is Senior Advisor Gro≈≤p Sustainable ·∏ûina≈áce and worked on green e‚Ç¶ergy ·∫¥nd ƒàlimat√© ri$k. Join us to m“æet Thina URL#oiw2018 URL   \n",
       "858  #WednesdayWisdom A^tifa calls +he right fascist when, in 4ll reality, th3y and the left are following the sa|V|e s<enari() as the Thi12d Reich: indoctrination of []ur youth, tryi^g to control minorities and a total lack of understanding or kn[]wled6e of hi$toI2y.#W4lkAway   \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "23         NOT         True  \n",
       "30         OFF         True  \n",
       "39         OFF         True  \n",
       "63         OFF         True  \n",
       "65         NOT         True  \n",
       "..         ...          ...  \n",
       "836        NOT         True  \n",
       "846        NOT         True  \n",
       "850        NOT         True  \n",
       "856        NOT         True  \n",
       "858        NOT         True  \n",
       "\n",
       "[86 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_test_offen_10_per[df_test_offen_10_per[\"Camouflaged\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b997c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2de690-0082-449e-8f17-d65dbc51da2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Table Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607930b5",
   "metadata": {},
   "source": [
    "Random examples extracted from the camouflaged data across different levels of complexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d353a431-481d-44f1-ab80-8fc32eb722f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 111.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 70.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 62.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 1848.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL                                                                                             \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  \n",
       "286  NOT        \n",
       "169  NOT        \n",
       "363  NOT        \n",
       "258  OFF        \n",
       "74   NOT        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 1 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@US√ÜR @≈≥SER @≈´S»áR Gun Control is nothing but p≈í∆üpl‚Ç§ C√òntr√òl. The left wants Gun CŒòntr òl ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North K»±r∆èa.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DƒØSARMTHÍ¨≤M #»ÉNTƒ©F‚àÜ  Givƒô JƒÉck here a participation trophy! ≈¨RL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTƒØFA ATTACKED C òPS, ƒÉRR∆∏ST3D AT RALLY IN DƒìNVƒóR, MEDƒ©‚àÄ BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#Cƒ≠ncƒ≠nnatƒ≠∆∏nq∆±ƒ≠r∆∏r steps into #G_n C√∂ntr√∂l F¬°NALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ≈¨RL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@∆±S∆êR Well she is Ch∆±ck Sch∆±mer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nƒ≠xŒòn. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @US√ÜR @≈≥SER @≈´S»áR Gun Control is nothing but p≈í∆üpl‚Ç§ C√òntr√òl. The left wants Gun CŒòntr òl ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North K»±r∆èa.   \n",
       "169  #DƒØSARMTHÍ¨≤M #»ÉNTƒ©F‚àÜ  Givƒô JƒÉck here a participation trophy! ≈¨RL                                                                                                                                                                                              \n",
       "363  6 ANTƒØFA ATTACKED C òPS, ƒÉRR∆∏ST3D AT RALLY IN DƒìNVƒóR, MEDƒ©‚àÄ BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #Cƒ≠ncƒ≠nnatƒ≠∆∏nq∆±ƒ≠r∆∏r steps into #G_n C√∂ntr√∂l F¬°NALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ≈¨RL                                                                                             \n",
       "74   @∆±S∆êR Well she is Ch∆±ck Sch∆±mer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nƒ≠xŒòn. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 2 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@'≈´'S'E'R @(_)≈ùE≈ó @U≈üE≈ô ƒù≈´‚Ç¶ CÍ¨Ωn‚Ä†`r0‚àü is nothing but )|)&gt;)e)≈è)|)&gt;)‚àü)e /C/o/Õ∑/‚Ç∏/r/o/ƒº. The left wants G≈©√± C√ònt–Ø ò| ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North |&lt;o–ØeƒÉ.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#D1≈õ…Ö≈ó]V[‚Ç∏HE]V[ #A|\\|Tƒ±FA  Gƒ±v‚ÑÆ ]ƒÖ¬©k here a participation trophy! \"U\"¬Æ\"L</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 √§N≈£I&lt;F√§ ATTACKED ƒâOP$, |4|≈ó|≈ó|E|œü|7|E|D AT RALLY IN DEŒ∑·πøE#R, MED!A BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#Cin/cin\\na≈ßi;En^qui‚Ñùe‚Ñù steps into #ƒ£≈©{\\} =C=≈è=≈ã=t=≈ó=≈è=‚Ñì FIN√§≈Ç≈ÇÍ≠ö. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. UR‚àü</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@U.5.∆πR Well she is }ƒç}h}∆±}ƒç}k ≈õ¬©ƒ•≈¨mer's cousin or niece or something so she is obviously part of the D≈â[ conspiracy against !N!ƒ≠!Í≠ô!o!n. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                   tweet  \\\n",
       "286  @'≈´'S'E'R @(_)≈ùE≈ó @U≈üE≈ô ƒù≈´‚Ç¶ CÍ¨Ωn‚Ä†`r0‚àü is nothing but )|)>)e)≈è)|)>)‚àü)e /C/o/Õ∑/‚Ç∏/r/o/ƒº. The left wants G≈©√± C√ònt–Ø ò| ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North |<o–ØeƒÉ.   \n",
       "169  #D1≈õ…Ö≈ó]V[‚Ç∏HE]V[ #A|\\|Tƒ±FA  Gƒ±v‚ÑÆ ]ƒÖ¬©k here a participation trophy! \"U\"¬Æ\"L                                                                                                                                                                                                              \n",
       "363  6 √§N≈£I<F√§ ATTACKED ƒâOP$, |4|≈ó|≈ó|E|œü|7|E|D AT RALLY IN DEŒ∑·πøE#R, MED!A BLACKOUT  URL                                                                                                                                                                                                    \n",
       "258  #Cin/cin\\na≈ßi;En^qui‚Ñùe‚Ñù steps into #ƒ£≈©{\\} =C=≈è=≈ã=t=≈ó=≈è=‚Ñì FIN√§≈Ç≈ÇÍ≠ö. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. UR‚àü                                                                                                         \n",
       "74   @U.5.∆πR Well she is }ƒç}h}∆±}ƒç}k ≈õ¬©ƒ•≈¨mer's cousin or niece or something so she is obviously part of the D≈â[ conspiracy against !N!ƒ≠!Í≠ô!o!n. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                            \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 3 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@UœüE‚Ñù @¬µS≈ìR @]≈≥]S]E]‚±§ Gun ‚ÑÇ√∂œÄ≈£r√∂ƒ∫ is nothing but +p+√´+Í¨Ω+p+‚Ñí+‚ÑÆ √ß[]‚Ç¶≈£¬Æ∆ü‚Ñí. The left wants gee‡∏ö≈Ü C&lt;&gt;‡∏ó‚Çúr&lt;&gt;l ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North K»±rea.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#[)ISA/2‚Ç•‚Ç∏HE‚Ç• #*A*N*T*I*|*#*A  Gƒ©ve Ja√ß|&lt; here a participation trophy! ≈¨RL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6  TI FAAN ATTACKED C≈çPS, +A+R+R+E+S+‚Ä†+E+[+) AT RALLY IN |)E‚Ñï|/E≈ï, MED|√Ç BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#+‚ÑÇ+√Ø+≈â+‚ÑÇ+√Ø+≈â+≈â+a+'+]+[+'+√Ø+E+≈â+q+u+√Ø+ùîë+e+ùîë steps into #Gun {≈èn}tr òl FIN√É|_|Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ≈©RL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@. ä.‚à´.√Ü.R Well she is Ch√ºcƒ∏ \\$\\c\\h\\u\\‚Ñ≥\\ƒï\\r's cousin or niece or something so she is obviously part of the ƒèN¬© conspiracy against /\\/ƒØœá»±/\\/. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                         tweet  \\\n",
       "286  @UœüE‚Ñù @¬µS≈ìR @]≈≥]S]E]‚±§ Gun ‚ÑÇ√∂œÄ≈£r√∂ƒ∫ is nothing but +p+√´+Í¨Ω+p+‚Ñí+‚ÑÆ √ß[]‚Ç¶≈£¬Æ∆ü‚Ñí. The left wants gee‡∏ö≈Ü C<>‡∏ó‚Çúr<>l ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North K»±rea.   \n",
       "169  #[)ISA/2‚Ç•‚Ç∏HE‚Ç• #*A*N*T*I*|*#*A  Gƒ©ve Ja√ß|< here a participation trophy! ≈¨RL                                                                                                                                                                                                  \n",
       "363  6  TI FAAN ATTACKED C≈çPS, +A+R+R+E+S+‚Ä†+E+[+) AT RALLY IN |)E‚Ñï|/E≈ï, MED|√Ç BLACKOUT  URL                                                                                                                                                                                      \n",
       "258  #+‚ÑÇ+√Ø+≈â+‚ÑÇ+√Ø+≈â+≈â+a+'+]+[+'+√Ø+E+≈â+q+u+√Ø+ùîë+e+ùîë steps into #Gun {≈èn}tr òl FIN√É|_|Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ≈©RL                                                                                  \n",
       "74   @. ä.‚à´.√Ü.R Well she is Ch√ºcƒ∏ \\$\\c\\h\\u\\‚Ñ≥\\ƒï\\r's cousin or niece or something so she is obviously part of the ƒèN¬© conspiracy against /\\/ƒØœá»±/\\/. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- AugLy --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USE;R @U;SER ;@USE;R Gu;n Co;ntro;l is; not;hing; but; peo;ple ;cont;rol.; The; lef;t wa;nts ;gun ;cont;rol ;ONLY; bec;ause; the;y do; NOT; wan;t us; lit;tle ;guys; arm;ed &amp;;amp;; abl;e to; def;end ;ours;elve;s. M;akes; it ;hard;er t;o ge;t us; beh;ind ;barb;ed-w;ire ;fenc;es a;s th;ey s;aw i;n No;rth ;Kore;a.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CŒØncin‚ìùa‚ç°i‚ÑØnqœãirer steps into#Gun Control FINALLY. Took a#MassShhoting on their damn door step fo·∂â ·π™heir#editors t·ªû wak“ø ·ª≠p, ·ÉÆut better ƒªate than ≈äever. URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                        tweet  \\\n",
       "286  @USE;R @U;SER ;@USE;R Gu;n Co;ntro;l is; not;hing; but; peo;ple ;cont;rol.; The; lef;t wa;nts ;gun ;cont;rol ;ONLY; bec;ause; the;y do; NOT; wan;t us; lit;tle ;guys; arm;ed &;amp;; abl;e to; def;end ;ours;elve;s. M;akes; it ;hard;er t;o ge;t us; beh;ind ;barb;ed-w;ire ;fenc;es a;s th;ey s;aw i;n No;rth ;Kore;a.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                                                                                            \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                                                                                   \n",
       "258  #CŒØncin‚ìùa‚ç°i‚ÑØnqœãirer steps into#Gun Control FINALLY. Took a#MassShhoting on their damn door step fo·∂â ·π™heir#editors t·ªû wak“ø ·ª≠p, ·ÉÆut better ƒªate than ≈äever. URL                                                                                                                                                              \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                                                                             \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 10% Leet ####\n",
    "toy_test = df_test_offen.sample(15, random_state=2)\n",
    "\n",
    "# resiliance_level = \"Level_3\"\n",
    "# resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "\n",
    "# augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "#         extractor_type=\"yake\",\n",
    "#         max_top_n=20,\n",
    "#         leet_punt_prb=0.9,\n",
    "#         leet_change_prb=0.5,\n",
    "#         leet_change_frq=0.8,\n",
    "#         leet_uniform_change=0.6,\n",
    "#         punt_hyphenate_prb=0.7,\n",
    "#         punt_uniform_change_prb=0.95,\n",
    "#         punt_word_splitting_prb=0.8,\n",
    "#         method=resiliance_intermediate,\n",
    "# )\n",
    "\n",
    "resiliance_level = \"Level_1.1\"\n",
    "resiliance_easy = [\"basic_leetspeak\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.8,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.5,\n",
    "        method=resiliance_easy,\n",
    ")\n",
    "\n",
    "# Create a test dataframe with 10% of leeted tweets\n",
    "df_level_1 = create_leet_augmenter_df(\n",
    "    df_ori=toy_test, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "resiliance_level = \"Level_2.1\"\n",
    "resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        method=resiliance_intermediate,\n",
    ")\n",
    "\n",
    "df_level_2 = create_leet_augmenter_df(\n",
    "    df_ori=toy_test, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "resiliance_level = \"Level_3.1\"\n",
    "resiliance_advanced = [\"advanced_leetspeak\", \"punct_camo\", \"inv_camo\"]\n",
    "\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5,\n",
    "        method=resiliance_advanced,\n",
    ")\n",
    "\n",
    "df_level_3 = create_leet_augmenter_df(\n",
    "    df_ori=toy_test, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "resiliance_level = \"AugLy\"\n",
    "\n",
    "########### 10% ############\n",
    "df_augly = create_augly_augmentation(\n",
    "    df_ori=toy_test, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "# toy_test = df_test_offen.sample(15, random_state=42)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "display(toy_test.head())\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 1 --------------\")\n",
    "display(df_level_1.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 2 --------------\")\n",
    "display(df_level_2.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 3 --------------\")\n",
    "display(df_level_3.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- AugLy --------------\")\n",
    "display(df_augly.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5851a9-2dd0-4397-ac69-42e1e6a29237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL                                                                                             \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  \n",
       "286  NOT        \n",
       "169  NOT        \n",
       "363  NOT        \n",
       "258  OFF        \n",
       "74   NOT        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_test = df_test_offen.sample(15, random_state=2)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "display(toy_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c57d2b21-a828-419f-97ff-efe28cfa93d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL                                                                                             \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  \n",
       "286  NOT        \n",
       "169  NOT        \n",
       "363  NOT        \n",
       "258  OFF        \n",
       "74   NOT        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 1 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@∆±S‚Ç§R @≈≥SER @≈≥SER G_n C≈ëntr≈ël is nothing but pe√òple Control. The l√´ft wants Gun C≈çntr≈çl ONLY because they do NOT want us little g≈©ys Œ±rm‚Ç§d &amp;amp; able to d3f3nd ourselves. M√§k√´s it hƒÖrd3r to get us behind b»Årbed-w√≠re fÍ¨≤ncƒós as they saw in N0rth K≈çr∆èƒÖ.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISƒÉRMTHƒìM #√§NTƒ≠F√§  Gƒ©vƒô J√§ck here a p‚àÜrt√Øc!p‚àÜt!0n tr≈èphy! ≈¨RL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANT1FA ‚àÄTT‚àÄCKED C∆üPS, ARRESTED AT R–îLLY IN D∆èNV3R, M‚Ç¨DI‚àÄ BL@CK ò‡∏öT  ≈©RL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#C1nc1nnŒ±t1ƒõnq‡∏ö1rƒõr st∆∏ps into #G√ºn C&lt;&gt;ntr&lt;&gt;l F√≠NALLY. Took a #M–îssShh∆ütƒØng on their d…Ömn d≈è≈èr st∆êp for their #≈ídƒØt&lt;&gt;rs to wƒÅk∆è up, but better l»Åte than never. ≈¨RL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@≈≥SER Well she is Ch≈≥ck Sch_mƒór's c ò≈©s1n or nƒØ√Ücƒì or something so she is obviously p‚±•rt of the DNC c ònsp!r4cy against Nixon. Or part of the D‚Ç¨‚Ç¨p St‚àÜt≈í. Or a Z¬°&lt;&gt;n¬°st pl√∂t. Or 'Big c&lt;&gt;m√Üdy'. Or whatever the h∆êll t»±d‚±•y's c»±nsp√≠r»Écy th∆è≈ëry is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @∆±S‚Ç§R @≈≥SER @≈≥SER G_n C≈ëntr≈ël is nothing but pe√òple Control. The l√´ft wants Gun C≈çntr≈çl ONLY because they do NOT want us little g≈©ys Œ±rm‚Ç§d &amp; able to d3f3nd ourselves. M√§k√´s it hƒÖrd3r to get us behind b»Årbed-w√≠re fÍ¨≤ncƒós as they saw in N0rth K≈çr∆èƒÖ.   \n",
       "169  #DISƒÉRMTHƒìM #√§NTƒ≠F√§  Gƒ©vƒô J√§ck here a p‚àÜrt√Øc!p‚àÜt!0n tr≈èphy! ≈¨RL                                                                                                                                                                                              \n",
       "363  6 ANT1FA ‚àÄTT‚àÄCKED C∆üPS, ARRESTED AT R–îLLY IN D∆èNV3R, M‚Ç¨DI‚àÄ BL@CK ò‡∏öT  ≈©RL                                                                                                                                                                                     \n",
       "258  #C1nc1nnŒ±t1ƒõnq‡∏ö1rƒõr st∆∏ps into #G√ºn C<>ntr<>l F√≠NALLY. Took a #M–îssShh∆ütƒØng on their d…Ömn d≈è≈èr st∆êp for their #≈ídƒØt<>rs to wƒÅk∆è up, but better l»Åte than never. ≈¨RL                                                                                          \n",
       "74   @≈≥SER Well she is Ch≈≥ck Sch_mƒór's c ò≈©s1n or nƒØ√Ücƒì or something so she is obviously p‚±•rt of the DNC c ònsp!r4cy against Nixon. Or part of the D‚Ç¨‚Ç¨p St‚àÜt≈í. Or a Z¬°<>n¬°st pl√∂t. Or 'Big c<>m√Üdy'. Or whatever the h∆êll t»±d‚±•y's c»±nsp√≠r»Écy th∆è≈ëry is.             \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 2 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@#U#≈ü#»á#R @USƒï¬Æ @U≈õ∆πR ƒü(_)|\\| √ßo≈Ü·Çµrol is nothing but pe&lt;&gt;ple ,ƒá,√∂,|,\\,|,t,≈ó,√∂,l. The left wants G(_)Œ∑ √ßŒòn‚Çú–Ø ò| ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Kore»Å.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DI5ARM·µóH≈ìM #4‚Ñï«Ç1œù4  $ƒ°$ƒ±$v$e JacùïÇ here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 :A:N:Õ≥:I:≈ø:A ATTACKED C√ò‚Ñô¬ß, √§R`RE≈°‚Ä†Eƒé AT RALLY IN |)E≈à€∑ER, M‚ÑÆ|)ƒØA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#[¬°n[¬°n-nƒÉ≈ß¬°En-q≈≥¬°≈óe≈ó steps into #G(_)n ƒáon≈¢≈óol F1‚ÑïALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. UR≈Ç</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@≈¨$E–Ø Well she is Cƒ•ucƒ∑ #S#[#h#≈≥#m#e#r's cousin or niece or something so she is obviously part of the DNC conspiracy against Ni&gt;&lt;on. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                tweet  \\\n",
       "286  @#U#≈ü#»á#R @USƒï¬Æ @U≈õ∆πR ƒü(_)|\\| √ßo≈Ü·Çµrol is nothing but pe<>ple ,ƒá,√∂,|,\\,|,t,≈ó,√∂,l. The left wants G(_)Œ∑ √ßŒòn‚Çú–Ø ò| ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Kore»Å.   \n",
       "169  #DI5ARM·µóH≈ìM #4‚Ñï«Ç1œù4  $ƒ°$ƒ±$v$e JacùïÇ here a participation trophy! URL                                                                                                                                                                                                                \n",
       "363  6 :A:N:Õ≥:I:≈ø:A ATTACKED C√ò‚Ñô¬ß, √§R`RE≈°‚Ä†Eƒé AT RALLY IN |)E≈à€∑ER, M‚ÑÆ|)ƒØA BLACKOUT  URL                                                                                                                                                                                                  \n",
       "258  #[¬°n[¬°n-nƒÉ≈ß¬°En-q≈≥¬°≈óe≈ó steps into #G(_)n ƒáon≈¢≈óol F1‚ÑïALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. UR≈Ç                                                                                                               \n",
       "74   @≈¨$E–Ø Well she is Cƒ•ucƒ∑ #S#[#h#≈≥#m#e#r's cousin or niece or something so she is obviously part of the DNC conspiracy against Ni><on. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                             \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 3 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USE|` @{‚îî{‚îò{S{E{R @≈©$E2 \"G\"u\"|\"\\\"| \"t≈ïo1Con is nothing but _peo_p_le /C/≈è/n/t/r/Í¨Ω/|. The left wants {,un @C@[@]@n@t@r@√∂@ƒº ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North 1&lt;or∆êa.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#ƒêƒ±S√ÉR…±T‚±®E‚Ç• #‚àÄN]TI]F‚àÄ  (+ƒØ‚àöe Jac/&lt; here a participation trophy! &amp;≈´&amp;R&amp;L</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ‚àÜN≈¶I|F‚àÜ ATTACKED COP≈ü, AR^RƒìS»∂ƒìD AT RALLY IN DEŒ∑€∑ER, DIA^^ƒõ_ BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#¬©||\\|¬©||\\||\\|ƒÅt|E|\\|qu|‚Ñùe‚Ñù steps into #~G~ ä~[~\\~] !ƒã!o!n!·Çµ!≈ï!o!¬£ }‚Ç£}ƒ≠}{}\\}}}A}L}L}Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. √ºR≈Ç</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@≈≠$ER Well she is ‚ÜÑhu&lt;ƒ∂ ≈°‚ÜÑhu^^»ár's cousin or niece or something so she is obviously part of the \\D\\N\\C conspiracy against  ≈Ñ i œá œÜ ≈Ñ. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                              tweet  \\\n",
       "286  @USE|` @{‚îî{‚îò{S{E{R @≈©$E2 \"G\"u\"|\"\\\"| \"t≈ïo1Con is nothing but _peo_p_le /C/≈è/n/t/r/Í¨Ω/|. The left wants {,un @C@[@]@n@t@r@√∂@ƒº ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North 1<or∆êa.   \n",
       "169  #ƒêƒ±S√ÉR…±T‚±®E‚Ç• #‚àÄN]TI]F‚àÄ  (+ƒØ‚àöe Jac/< here a participation trophy! &≈´&R&L                                                                                                                                                                                                                           \n",
       "363  6 ‚àÜN≈¶I|F‚àÜ ATTACKED COP≈ü, AR^RƒìS»∂ƒìD AT RALLY IN DEŒ∑€∑ER, DIA^^ƒõ_ BLACKOUT  URL                                                                                                                                                                                                                     \n",
       "258  #¬©||\\|¬©||\\||\\|ƒÅt|E|\\|qu|‚Ñùe‚Ñù steps into #~G~ ä~[~\\~] !ƒã!o!n!·Çµ!≈ï!o!¬£ }‚Ç£}ƒ≠}{}\\}}}A}L}L}Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. √ºR≈Ç                                                                                                \n",
       "74   @≈≠$ER Well she is ‚ÜÑhu<ƒ∂ ≈°‚ÜÑhu^^»ár's cousin or niece or something so she is obviously part of the \\D\\N\\C conspiracy against  ≈Ñ i œá œÜ ≈Ñ. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                                          \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- AugLy --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@ US3R @ USER @ USER Gun Control is ^oth|ng but p3ople control. T/-/e left wants gun control O^LY because they dD ^OT want us little 9uys armed &amp; amp; able to |)efend ()urse|_ves. Makes it harder to get us behind barbed-wire fences as th3y saw i^ North Kor3a.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM#ANTIF4 Give Jack her3 a participation trophy! U/2L</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#Cin!cinn!atiE!nqui!rer !step!s in!to #!Gun !Cont!rol !FINA!LLY.! Too!k a !#Mas!sShh!otin!g on! the!ir d!amn !door! ste!p fo!r th!eir !#edi!tors! to !wake! up,! but! bet!ter !late! tha!n ne!ver.! URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USE.R We.ll s.he i.s Ch.uck .Schu.mer'.s co.usin. or .niec.e or. som.ethi.ng s.o sh.e is. obv.ious.ly p.art .of t.he D.NC c.onsp.irac.y ag.ains.t Ni.xon.. Or .part. of .the .Deep. Sta.te. .Or a. Zio.nist. plo.t. O.r 'B.ig c.omed.y'. .Or w.hate.ver .the .hell. tod.ay's. con.spir.acy .theo.ry i.s.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         tweet  \\\n",
       "286  @ US3R @ USER @ USER Gun Control is ^oth|ng but p3ople control. T/-/e left wants gun control O^LY because they dD ^OT want us little 9uys armed & amp; able to |)efend ()urse|_ves. Makes it harder to get us behind barbed-wire fences as th3y saw i^ North Kor3a.                                         \n",
       "169  #DISARMTHEM#ANTIF4 Give Jack her3 a participation trophy! U/2L                                                                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                                                                    \n",
       "258  #Cin!cinn!atiE!nqui!rer !step!s in!to #!Gun !Cont!rol !FINA!LLY.! Too!k a !#Mas!sShh!otin!g on! the!ir d!amn !door! ste!p fo!r th!eir !#edi!tors! to !wake! up,! but! bet!ter !late! tha!n ne!ver.! URL                                                                                                     \n",
       "74   @USE.R We.ll s.he i.s Ch.uck .Schu.mer'.s co.usin. or .niec.e or. som.ethi.ng s.o sh.e is. obv.ious.ly p.art .of t.he D.NC c.onsp.irac.y ag.ains.t Ni.xon.. Or .part. of .the .Deep. Sta.te. .Or a. Zio.nist. plo.t. O.r 'B.ig c.omed.y'. .Or w.hate.ver .the .hell. tod.ay's. con.spir.acy .theo.ry i.s.   \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# toy_test = df_test_offen.sample(15, random_state=42)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "display(toy_test.head())\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 1 --------------\")\n",
    "display(df_level_1.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 2 --------------\")\n",
    "display(df_level_2.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 3 --------------\")\n",
    "display(df_level_3.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- AugLy --------------\")\n",
    "display(df_augly.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7552c5-028c-44a3-877c-aec577050d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NAIVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83718906-17e1-464d-91c2-36f75164cc77",
   "metadata": {},
   "source": [
    "- [x] Naive\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best ~/work/Code/WordCamouflage_Resiliance/code/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best/test_result.json\n",
    "    \n",
    "python -m spacy evaluate ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best ~/work/Code/WordCamouflage_Resiliance/code/Spacy_Data/Constraint/Leet_Data/Level_3.2/100_per/test.spacy  --gpu-id 1 --output ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best/test_100_level_3.2_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    \n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    \n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c5094-4476-4f68-842b-e3e403ce8bb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 10_leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c64ac1-604b-425d-b2ba-a5ed1364dfd1",
   "metadata": {},
   "source": [
    "- [x] 10_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>    \n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "       ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd226c82-c11c-47b7-aaae-442f1d4d40e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 25 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8865f2-44d4-4434-a344-2c1a1c358bc3",
   "metadata": {},
   "source": [
    "- [ ] 25_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52828a32-846b-4459-a2ff-5bb1064a825c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 50 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be0994-0f6a-4b0d-9cb4-7d94f8213f3d",
   "metadata": {},
   "source": [
    "- [ ] 50_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf1e0d-6bdb-4396-a40a-cac4d8f80d31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 75 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714d0cc-0e99-4764-bdca-56e1de1e942d",
   "metadata": {},
   "source": [
    "- [ ] 75_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d925a87-a59a-4779-afcc-376ff3b34d0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 100 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03aea9-d645-4e9c-bf34-d7ddb287031b",
   "metadata": {},
   "source": [
    "- [ ] 100_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b5929-aa6a-49ec-977e-0251d31f0dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 10_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a89ba-74b5-45fd-a7b8-16f0daf30003",
   "metadata": {},
   "source": [
    "- [ ] 10_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927942aa-c8d8-4db9-975a-483545ec01bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 25_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7ebe0-777c-4835-9e3d-0c5c9b44894a",
   "metadata": {},
   "source": [
    "- [ ] 25_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00868dd8-7f80-4ba7-aef4-0c0df98aede6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 50_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e3951-63b0-4717-9f05-5121049dd6d7",
   "metadata": {},
   "source": [
    "- [ ] 50_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382675d8-840e-4041-94a5-132c547eca33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 75_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2694e-854a-4118-9fb5-1804bfa6d94d",
   "metadata": {},
   "source": [
    "- [ ] 75_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589474f7-168a-41e9-8ccf-a59f75a9b959",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 100_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b59a80-7d50-45c6-a9c8-1a4d581f4b41",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- [ ] 100_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CamouflageKernel",
   "language": "python",
   "name": "camouflagekernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ac8b2602792a0e1037910f4d8758f49024557e1c1607b4d9be870b3f2a26f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
