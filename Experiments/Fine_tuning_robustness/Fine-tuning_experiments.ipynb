{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077f781-c0f6-46e9-b97c-bfcfced6b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! This way you can navigate through $home an get the path to the NAS data\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "home = str(Path.home())\n",
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc14990c-bbd4-4abd-b219-634cbdfa2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 08:11:00.368829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-04 08:11:00.654352: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-04 08:11:00.710962: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-04 08:11:01.753536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-04 08:11:01.753644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-04 08:11:01.753657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy version: 3.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alvaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alvaro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy, copy\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from codetiming import Timer\n",
    "import pyleetspeak\n",
    "# print(f\"Pyleetspeak version: {pyleetspeak.__version__}\")\n",
    "print(f\"Spacy version: {spacy.__version__}\")\n",
    "\n",
    "\n",
    "from pyleetspeak.pyleetspeak import WordCamouflage_Augmenter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e9d1f-92aa-4420-b0b1-e7f5dca9cd3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fd258e-8ab0-4b8f-94b9-944e22531ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(data_tuples, nlp, labels):\n",
    "    \"\"\"_summary_\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for text, label in tqdm(nlp.pipe(data_tuples, as_tuples=True), total = len(data_tuples), desc = \"Making docs\"):\n",
    "        doc = nlp(text)\n",
    "\n",
    "        for l in labels:\n",
    "            # Hay que hacer todos los labels\n",
    "            doc.cats[l] = label == l   \n",
    "\n",
    "        # put them into a nice list\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def leet_data(text, generator):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    NER_data, ori_data = generator.generate_data(\n",
    "            sentence=text\n",
    "            # important_kws = [r\"\\bpfizer\\b\", r\"control\\b\", r\"vacuna\\b\", r\"vaccines\\b\"],\n",
    "        )\n",
    "\n",
    "    leet_text, _ =  NER_data[0]\n",
    "    return leet_text\n",
    "\n",
    "\n",
    "# function to create dataframes witrh a percentage leet tweets\n",
    "def create_leet_df(df_ori, frac, generator, column_to_leet):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        frac (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "        column_to_leet (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_leeted = copy(df_ori)\n",
    "    df_leeted[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract fraction to leet\n",
    "    df_to_leet = df_leeted.sample(frac=frac, random_state=42)\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_leet[column_to_leet] = df_to_leet[column_to_leet].progress_apply(leet_data,  generator=generator_EN)\n",
    "    df_to_leet[\"Camouflaged\"] = True\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(f\"Nan values in '{column_to_leet}' column: \", df_to_leet[column_to_leet].isna().sum())\n",
    "\n",
    "    # Substitute the original rows by the leeted version\n",
    "    cols = list(df_leeted.columns) \n",
    "    df_leeted.loc[df_leeted.index.isin(df_to_leet.index), cols] = df_to_leet[cols]    \n",
    "\n",
    "    display( df_leeted.groupby(\"Camouflaged\").count() ) \n",
    "    \n",
    "    return df_leeted\n",
    "\n",
    "\n",
    "def create_augmented_df(df_ori, frac, generator):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df_ori (_type_): _description_\n",
    "        frac (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_augmented = copy(df_ori)\n",
    "    df_augmented[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract different fraction to leet \n",
    "    df_to_augment = df_augmented.sample(frac=frac, random_state=42)\n",
    "    df_to_augment_2 = df_augmented.sample(frac=frac, random_state=4)\n",
    "    df_to_augment_3 = df_augmented.sample(frac=frac, random_state=12)\n",
    "\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_augment[\"tweet\"] = df_to_augment[\"tweet\"].progress_apply(leet_data,  generator=generator)\n",
    "    df_to_augment[\"Camouflaged\"] = True\n",
    "\n",
    "    df_to_augment_2[\"tweet\"] = df_to_augment_2[\"tweet\"].progress_apply(leet_data,  generator=generator)\n",
    "    df_to_augment_2[\"Camouflaged\"] = True\n",
    "\n",
    "    df_to_augment_3[\"tweet\"] = df_to_augment_3[\"tweet\"].progress_apply(leet_data,  generator=generator)\n",
    "    df_to_augment_3[\"Camouflaged\"] = True\n",
    "\n",
    "\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(\"Nan values in df_train_offen_to_leet['tweet'] column: \", df_to_augment[\"tweet\"].isna().sum())\n",
    "    print(\"Nan values in df_train_offen_to_leet['tweet'] column: \", df_to_augment_2[\"tweet\"].isna().sum())\n",
    "    print(\"Nan values in df_train_offen_to_leet['tweet'] column: \", df_to_augment_3[\"tweet\"].isna().sum())\n",
    "\n",
    "    # Add the augmented tweets to the original dataframe\n",
    "    df_augmented = pd.concat([df_augmented, df_to_augment, df_to_augment_2, df_to_augment_3], ignore_index=True)\n",
    "    # suffle the dataframe\n",
    "    df_augmented = df_augmented.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "    display( df_augmented.groupby(\"Camouflaged\").count() ) \n",
    "    \n",
    "    return df_augmented\n",
    "\n",
    "\n",
    "\n",
    "# Create a function to save pandas dataframe to spacy binary file\n",
    "def pd_2_spacy(df_train, df_dev, df_test, train_output_path, dev_output_path, test_output_path, labels, lang=\"en\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df_train (_type_): _description_\n",
    "        df_dev (_type_): _description_\n",
    "        df_test (_type_): _description_\n",
    "        train_output_path (_type_): _description_\n",
    "        dev_output_path (_type_): _description_\n",
    "        test_output_path (_type_): _description_\n",
    "        lang (str, optional): _description_. Defaults to \"en\".\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Spact empty model\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    if df_train is not None:\n",
    "        # tuple of tuples. Each nested tuple is (Tweet, Label)\n",
    "        train_data_tuples = tuple(df_train.iloc[:, [0,1]].itertuples(index=False, name=None))\n",
    "                \n",
    "        # Make spacy DocBin\n",
    "        train_docs = make_docs(train_data_tuples, nlp, labels)\n",
    "\n",
    "        # Make outpath directory with Pathlib\n",
    "        Path(train_output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # save to binary file \n",
    "        train_doc_bin = DocBin(docs=train_docs)\n",
    "        train_doc_bin.to_disk(train_output_path)\n",
    "        print(f\"Processed Train {len(train_data_tuples)} documents: {train_output_path}\")        \n",
    "\n",
    "    if df_dev is not None:\n",
    "        dev_data_tuples = tuple(df_dev.iloc[:, [0,1]].itertuples(index=False, name=None))\n",
    "        dev_docs = make_docs(dev_data_tuples, nlp, labels)\n",
    "        Path(dev_output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        dev_doc_bin = DocBin(docs=dev_docs)\n",
    "        dev_doc_bin.to_disk(dev_output_path)\n",
    "        print(f\"Processed Dev {len(dev_data_tuples)} documents: {dev_output_path}\")\n",
    "\n",
    "    if df_test is not None:\n",
    "        test_data_tuples = tuple(df_test.iloc[:, [0,1]].itertuples(index=False, name=None))\n",
    "        test_docs = make_docs(test_data_tuples, nlp, labels)\n",
    "        Path(test_output_path).parent.mkdir(parents=True, exist_ok=True)   \n",
    "\n",
    "        test_doc_bin = DocBin(docs=test_docs)\n",
    "        test_doc_bin.to_disk(test_output_path)\n",
    "        print(f\"Processed Test {len(test_data_tuples)} documents: {test_output_path}\")    \n",
    "\n",
    "\n",
    "def leet_data_augmenter(text, augmenter):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    leet_text = augmenter.transform(\n",
    "            text\n",
    "        )\n",
    "    return leet_text\n",
    "\n",
    "\n",
    "# function to create dataframes witrh a percentage leet tweets\n",
    "def create_leet_augmenter_df(df_ori, frac, augmenter, column_to_leet):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        frac (_type_): _description_\n",
    "        generator (_type_): _description_\n",
    "        column_to_leet (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_leeted = copy(df_ori)\n",
    "    df_leeted[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract fraction to leet\n",
    "    df_to_leet = df_leeted.sample(frac=frac, random_state=42)\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_leet[column_to_leet] = df_to_leet[column_to_leet].progress_apply(leet_data_augmenter,  augmenter=augmenter)\n",
    "    df_to_leet[\"Camouflaged\"] = True\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(f\"Nan values in '{column_to_leet}' column: \", df_to_leet[column_to_leet].isna().sum())\n",
    "\n",
    "    # Substitute the original rows by the leeted version\n",
    "    cols = list(df_leeted.columns) \n",
    "    df_leeted.loc[df_leeted.index.isin(df_to_leet.index), cols] = df_to_leet[cols]    \n",
    "\n",
    "    display( df_leeted.groupby(\"Camouflaged\").count() ) \n",
    "    \n",
    "    return df_leeted\n",
    "\n",
    "\n",
    "\n",
    "# Define la función random_augmenter\n",
    "def random_augly_augmenter(input_text):\n",
    "    # define augmenters\n",
    "    aug_ReplaceSimilar = textaugs.ReplaceSimilarChars(p=0.4)\n",
    "    aug_ReplaceSimUnicode = textaugs.ReplaceSimilarUnicodeChars(p=0.4)\n",
    "    aug_InsertPunctuation = textaugs.InsertPunctuationChars(granularity=\"all\", cadence=4, p=0.4)\n",
    "\n",
    "    aug_ReplaceSim_Punct = textaugs.Compose(\n",
    "        transforms=[\n",
    "            textaugs.ReplaceSimilarChars(p=0.4),\n",
    "            textaugs.InsertPunctuationChars(granularity=\"all\", cadence=4, p=1)\n",
    "        ],\n",
    "        p=0.4\n",
    "    )\n",
    "\n",
    "    aug_ReplaceSimUnicode_Punct = textaugs.Compose(\n",
    "        transforms=[\n",
    "            textaugs.ReplaceSimilarUnicodeChars(\n",
    "                p=1,\n",
    "            ),\n",
    "            textaugs.InsertPunctuationChars(granularity=\"all\", cadence=4, p=0.4)\n",
    "        ],\n",
    "        p=1,\n",
    "    )\n",
    "\n",
    "    augmenters = [\n",
    "        aug_ReplaceSimilar,\n",
    "        aug_ReplaceSimUnicode,\n",
    "        aug_InsertPunctuation,\n",
    "        aug_ReplaceSim_Punct,\n",
    "        aug_ReplaceSimUnicode_Punct,\n",
    "    ]\n",
    "\n",
    "    random_augmenter = rng.choice(\n",
    "        augmenters, replace=True, \n",
    "        # p=[0.5, 0.2, 0.12, 0.12, 0.06]\n",
    "    )\n",
    "    leet_text = random_augmenter(input_text)\n",
    "    \n",
    "    if isinstance(leet_text, list):\n",
    "        # Convierte la lista en una cadena usando un separador (por ejemplo, un espacio)\n",
    "        leet_text = \" \".join(leet_text)\n",
    "    \n",
    "    return leet_text\n",
    "\n",
    "def create_augly_augmentation(df_ori, frac, augmenter, column_to_leet):\n",
    "    # Create a copy of the original dataframe for saving the leeted version\n",
    "    df_leeted = copy(df_ori)\n",
    "    df_leeted[\"Camouflaged\"] = False\n",
    "    \n",
    "    # Extract fraction to leet\n",
    "    df_to_leet = df_leeted.sample(frac=frac, random_state=42)\n",
    "\n",
    "    # Leet the tweets\n",
    "    df_to_leet[column_to_leet] = df_to_leet[column_to_leet].progress_apply(augmenter)\n",
    "    df_to_leet[\"Camouflaged\"] = True\n",
    "\n",
    "    # count nan values in df_train_offen_to_leet[\"tweet\"] column\n",
    "    print(f\"Nan values in '{column_to_leet}' column: \", df_to_leet[column_to_leet].isna().sum())\n",
    "\n",
    "    # Substitute the original rows by the leeted version\n",
    "    cols = list(df_leeted.columns) \n",
    "    df_leeted.loc[df_leeted.index.isin(df_to_leet.index), cols] = df_to_leet[cols]    \n",
    "\n",
    "    display( df_leeted.groupby(\"Camouflaged\").count() )\n",
    "    \n",
    "    return df_leeted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4491a90-cc40-482a-952c-69b57ccdb2ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [OffenSenEval 2019](https://competitions.codalab.org/competitions/20011)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f271d-3038-474a-8fa2-b25654f250dc",
   "metadata": {},
   "source": [
    "Sub-task A: Offensive or not\n",
    "\n",
    "In this sub-task we are interested in the identification of offensive posts and posts containing any form of (untargeted) profanity. In this sub-task there are **2 categories** in which the tweet could be classified -\n",
    "\n",
    "- **Not Offensive** - This post does not contain offense or profanity.  Non-offensive posts do not include any form of offense or profanity.\n",
    "\n",
    "- **Offensive** - This post contains offensive language or a targeted (veiled or direct) offense.  In our annotation, we label a post as offensive if it  contains any form of non-acceptable language (profanity) or a targeted offense which can be veiled or direct. To sum up this category includes insults, threats, and posts containing profane language and swear words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b0e4e-3324-413e-90a5-afc37bb8ee14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Pandas data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af362b0",
   "metadata": {},
   "source": [
    "Load the orignal non-camouflage data and transform it to spacy data format for Naive models training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff19dda-5a72-4b00-af55-2200da7c747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtask_a</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT</th>\n",
       "      <td>7933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF</th>\n",
       "      <td>3953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet\n",
       "subtask_a       \n",
       "NOT         7933\n",
       "OFF         3953"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtask_a</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT</th>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF</th>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet\n",
       "subtask_a       \n",
       "NOT          882\n",
       "OFF          439"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT</th>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet\n",
       "test_label       \n",
       "NOT           620\n",
       "OFF           240"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Temporal Train\n",
    "# train_offen_path = Path(home).joinpath(\"data/WordCamouflage_Resiliance/Datasets/OffensEval_2019/OffensEval_2019/OLIDv1.0/olid-training-v1.0.tsv\")\n",
    "train_offen_path = Path(home).joinpath(\"work/Code/WordCamouflage_Resiliance/code/tmp_data/Offen/olid-training-v1.0.tsv\")\n",
    "\n",
    "df_train_tmp_offen = pd.read_csv( train_offen_path, sep = \"\\t\").drop(labels = [\"id\", \"subtask_b\", \"subtask_c\"], axis = 1)\n",
    "\n",
    "# drop duplicates\n",
    "df_train_tmp_offen = df_train_tmp_offen.drop_duplicates(subset = [\"tweet\"])\n",
    "\n",
    "labels_offen = list(df_train_tmp_offen[\"subtask_a\"].unique())\n",
    "\n",
    "# Split into Train and Dev\n",
    "df_train_offen, df_dev_offen = train_test_split(\n",
    "    df_train_tmp_offen,\n",
    "    random_state=42,\n",
    "    test_size=0.1,\n",
    "    stratify=df_train_tmp_offen[\"subtask_a\"].to_numpy(),\n",
    ")\n",
    "\n",
    "\n",
    "# Load Test Data and Label\n",
    "test_data_offen_path = Path(home).joinpath(\"work/Code/WordCamouflage_Resiliance/code/tmp_data/Offen/testset-levela.tsv\")\n",
    "test_label_offen_path = Path(home).joinpath(\"work/Code/WordCamouflage_Resiliance/code/tmp_data/Offen/labels-levela.csv\")\n",
    "\n",
    "df_test_data_offen = pd.read_csv( test_data_offen_path, sep = \"\\t\")\n",
    "df_test_label_offen = pd.read_csv( test_label_offen_path, sep = \",\", header=None, names=[\"id\", \"test_label\"])\n",
    "\n",
    "# Merge Test Data and Test Label\n",
    "df_test_offen = pd.merge(df_test_data_offen, df_test_label_offen, on=\"id\", how=\"outer\").drop(labels = [\"id\"], axis = 1)\n",
    "\n",
    "print(\"Train\")\n",
    "display(df_train_offen.groupby(\"subtask_a\").count())\n",
    "\n",
    "print(\"Dev\")\n",
    "display(df_dev_offen.groupby(\"subtask_a\").count())\n",
    "\n",
    "print(\"Test\")\n",
    "display(df_test_offen.groupby(\"test_label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f0915",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### From Pandas to Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff73b15-1670-48fe-86eb-2af93514dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train spacy, Dev spacy, Test spacy\n",
    "train_offen_output_path = \"./train_offen_toy.spacy\"\n",
    "dev_offen_output_path = \"./dev_offen_toy.spacy\"\n",
    "test_offen_output_path = \"./test_offen_toy.spacy\"\n",
    "\n",
    "# tuple of tuples. Each nested tuple is (Tweet, Label)\n",
    "train_offen_data_tuples = tuple(df_train_offen.itertuples(index=False, name=None))\n",
    "dev_offen_data_tuples = tuple(df_dev_offen.itertuples(index=False, name=None))\n",
    "test_offen_data_tuples = tuple(df_test_offen.itertuples(index=False, name=None))\n",
    "\n",
    "# Spact empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Make spacy DocBin\n",
    "train_offen_docs = make_docs(train_offen_data_tuples, nlp)\n",
    "dev_offen_docs = make_docs(dev_offen_data_tuples, nlp)\n",
    "test_offen_docs = make_docs(test_offen_data_tuples, nlp)\n",
    "\n",
    "# save to binary file \n",
    "train_offen_doc_bin = DocBin(docs=train_offen_docs)\n",
    "train_offen_doc_bin.to_disk(train_offen_output_path)\n",
    "print(f\"Processed Train {len(train_offen_data_tuples)} documents: {train_offen_output_path}\")\n",
    "\n",
    "dev_offen_doc_bin = DocBin(docs=dev_offen_docs)\n",
    "dev_offen_doc_bin.to_disk(dev_offen_output_path)\n",
    "print(f\"Processed Dev {len(dev_offen_data_tuples)} documents: {dev_offen_output_path}\")\n",
    "\n",
    "test_offen_doc_bin = DocBin(docs=test_offen_docs)\n",
    "test_offen_doc_bin.to_disk(test_offen_output_path)\n",
    "print(f\"Processed Test {len(test_offen_data_tuples)} documents: {test_offen_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7fc83",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Camouflage Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef181b",
   "metadata": {},
   "source": [
    "In the following cells we generate the data for model fitting with the _static_ strategy for the different levels of complecity, word camouflage ratio and instance camouflage ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdd862-e534-4090-b58c-4043f6bfe979",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Level 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213d5ac-2b8e-4bf6-9606-fec7120e2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_1.1\"\n",
    "resiliance_easy = [\"basic_leetspeak\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.8,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.5,\n",
    "        method=resiliance_easy,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ccfe5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Level 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a4ca7-2815-43be-8532-fbeb12ccfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_1.2\"\n",
    "resiliance_easy = [\"basic_leetspeak\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.8,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.5,\n",
    "        method=resiliance_easy,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278afdcd-9957-4b9d-99cb-1af0e5d66dda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576af59-96f9-45f0-805b-989af37ff4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_2.1\"\n",
    "resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        method=resiliance_intermediate,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfebf3f7-bfe4-4a3a-9480-9339591db058",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a3e97-a229-4f81-91e5-836b8ed8a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_2.2\"\n",
    "resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        method=resiliance_intermediate,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7a22d-ceeb-4f7c-bc89-506de74e1923",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a822c-4206-482c-8438-15ae34a85603",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_3.1\"\n",
    "resiliance_advanced = [\"advanced_leetspeak\", \"punct_camo\", \"inv_camo\"]\n",
    "\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5,\n",
    "        method=resiliance_advanced,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6124149",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Level 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d99c38-2493-4953-bd17-e090dac1c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_3.2\"\n",
    "resiliance_advanced = [\"advanced_leetspeak\", \"punct_camo\", \"inv_camo\"]\n",
    "\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5,\n",
    "        method=resiliance_advanced,\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41523f78-9815-4cb2-9a90-ca14fef89e57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Level Mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6bdef",
   "metadata": {},
   "source": [
    "Mixed camouflaged version data combining the different levels of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb0bfa-917f-4b9b-8acf-3e129280498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliance_level = \"Level_Mixed\"\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=20,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5\n",
    ")\n",
    "\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_leet_augmenter_df(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447877",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cc146-c051-4dfe-b64a-18b6dc86d43c",
   "metadata": {},
   "source": [
    "#### Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d9e17-c7b4-4342-b887-77da75d94a5a",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m spacy train  [config-path] --output [output-path] --gpu-id X --code [augmenter-script]\n",
    "```\n",
    "\n",
    "For example, training the BERT encoder-model in dynamic strategy with 10% of data instances camouflaged with mixed camouflaged. \n",
    "```bash\n",
    "python -m spacy train ~work/NLP-Camouflage-is-all-you-need/Experiments/Fine_tuning_robustness/configs/offen_semeval_10_dynamic_config.cfg --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-dynamic --gpu-id 0 --code functions.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66216dc-ea66-4522-b8f0-213c74b90875",
   "metadata": {},
   "source": [
    "#### Static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f68c2-b97f-4abb-bb0e-7521f56b6caf",
   "metadata": {},
   "source": [
    "In this case, the `config-path` track a config file where the data used is the one previously camouflaged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286e965-6a03-41f8-82d2-01a014cbfc83",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m spacy train  [config-path] --output [output-path] --gpu-id X \n",
    "```\n",
    "\n",
    "For example, training the BERT encoder-model in static strategy with 10% of data instances camouflaged with mixed camouflaged. \n",
    "```bash\n",
    "python -m spacy train ~work/NLP-Camouflage-is-all-you-need/Experiments/Fine_tuning_robustness/configs/offen_semeval_10_static_config.cfg --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-static --gpu-id 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be746e7f-744b-4fa5-9a41-1ae375a53cde",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20881359-3fe2-4258-8e4e-fa2e24bc00ab",
   "metadata": {},
   "source": [
    "#### Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580e40e-a8fc-4019-aef3-a198ddc65fa1",
   "metadata": {},
   "source": [
    "10%, Level 1 v2\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate [model-to-path] [test-data-path] --gpu-id X --output [output-results-path]\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-dynamic ~work/NLP-Camouflage-is-all-you-need/Spacy_Data/Offen_SemEval_2019/Leet_Data/Level_1.2/10_per/test.spacy --gpu-id 1 --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-dynamic/model-best/test_10_level_1.2_result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f017906-8fe1-4633-835d-73f3226f7245",
   "metadata": {},
   "source": [
    "#### Static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a748c7-5675-423b-adc5-4ba033923d06",
   "metadata": {},
   "source": [
    "10%, Level 1 v2\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate [model-to-path] [test-data-path] --gpu-id X --output [output-results-path]\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m spacy evaluate ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-static ~work/NLP-Camouflage-is-all-you-need/Spacy_Data/Offen_SemEval_2019/Leet_Data/Level_1.2/10_per/test.spacy --gpu-id 1 --output ~work/NLP-Camouflage-is-all-you-need/output_models/bert-base-uncased_offen-10-static/model-best/test_10_level_1.2_result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0d455-8957-4b2c-b6a5-78288795ea07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AugLy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a29dd",
   "metadata": {},
   "source": [
    "External validation with AugLy augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d53474a-04a1-4d65-992d-f8f16a298bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import augly.text as textaugs\n",
    "import numpy as np\n",
    "\n",
    "# Crea un generador con la semilla 42\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08be7e2-e9c6-45ce-8985-45ed3321a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1189/1189 [00:01<00:00, 853.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>10697</td>\n",
       "      <td>10697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False        10697      10697\n",
       "True          1189       1189"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 132/132 [00:00<00:00, 895.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         1189       1189\n",
       "True           132        132"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 86/86 [00:00<00:00, 755.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          774         774\n",
       "True            86          86"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:04<00:00, 2919.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 3887.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 2510.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2972/2972 [00:04<00:00, 726.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>8914</td>\n",
       "      <td>8914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2972</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         8914       8914\n",
       "True          2972       2972"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 330/330 [00:00<00:00, 786.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>991</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>330</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False          991        991\n",
       "True           330        330"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 215/215 [00:00<00:00, 823.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>645</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          645         645\n",
       "True           215         215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:04<00:00, 2454.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/25_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 3280.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/25_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 2185.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/25_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5943/5943 [00:07<00:00, 820.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5943</td>\n",
       "      <td>5943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>5943</td>\n",
       "      <td>5943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         5943       5943\n",
       "True          5943       5943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 660/660 [00:00<00:00, 820.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>661</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>660</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False          661        661\n",
       "True           660        660"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 430/430 [00:00<00:00, 842.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          430         430\n",
       "True           430         430"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:05<00:00, 2042.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/50_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 2490.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/50_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 1706.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/50_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 8914/8914 [00:10<00:00, 827.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2972</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>8914</td>\n",
       "      <td>8914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False         2972       2972\n",
       "True          8914       8914"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 991/991 [00:01<00:00, 860.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>330</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>991</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "False          330        330\n",
       "True           991        991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 645/645 [00:00<00:00, 813.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>645</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "False          215         215\n",
       "True           645         645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:07<00:00, 1629.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/75_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 1953.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/75_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 1374.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/75_per/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 11886/11886 [00:14<00:00, 839.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>11886</td>\n",
       "      <td>11886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "True         11886      11886"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1321/1321 [00:01<00:00, 852.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  subtask_a\n",
       "Camouflaged                  \n",
       "True          1321       1321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 860/860 [00:01<00:00, 813.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>860</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True           860         860"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 11886/11886 [00:08<00:00, 1370.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train 11886 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/100_per/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 1321/1321 [00:00<00:00, 1744.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dev 1321 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/100_per/dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making docs: 100% 860/860 [00:00<00:00, 1213.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test 860 documents: ./Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/100_per/test.spacy\n"
     ]
    }
   ],
   "source": [
    "resiliance_level = \"AugLy\"\n",
    "\n",
    "########### 10% ############\n",
    "df_train_offen_10_per = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_10_per = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_10_per= create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_10_per,\n",
    "df_dev = df_dev_offen_10_per, \n",
    "df_test = df_test_offen_10_per, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/10_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 25% ############\n",
    "df_train_offen_25_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.25, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_25_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.25, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_25_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.25, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 25% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_25_per_advanced_v2,\n",
    "df_dev = df_dev_offen_25_per_advanced_v2, \n",
    "df_test = df_test_offen_25_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/25_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 50% ############\n",
    "df_train_offen_50_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.5, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_50_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.5, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_50_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.5, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 50% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_50_per_advanced_v2,\n",
    "df_dev = df_dev_offen_50_per_advanced_v2, \n",
    "df_test = df_test_offen_50_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/50_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "########### 75% ############\n",
    "df_train_offen_75_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=0.75, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_75_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=0.75, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_75_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=0.75, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 75% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_75_per_advanced_v2,\n",
    "df_dev = df_dev_offen_75_per_advanced_v2, \n",
    "df_test = df_test_offen_75_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/75_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")\n",
    "\n",
    "\n",
    "########### 100% ############\n",
    "df_train_offen_100_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_train_offen, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_dev_offen_100_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_dev_offen, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "df_test_offen_100_per_advanced_v2 = create_augly_augmentation(\n",
    "    df_ori=df_test_offen, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "# Save the 100% leeted difficult dataframes to spacy binary files\n",
    "pd_2_spacy(df_train = df_train_offen_100_per_advanced_v2,\n",
    "df_dev = df_dev_offen_100_per_advanced_v2, \n",
    "df_test = df_test_offen_100_per_advanced_v2, \n",
    "train_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/train.spacy\", \n",
    "dev_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/dev.spacy\",\n",
    "test_output_path = f\"./Spacy_Data/Offen_SemEval_2019/Leet_Data/{resiliance_level}/100_per/test.spacy\",\n",
    "lang=\"en\",\n",
    "labels=labels_offen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13cbe93",
   "metadata": {},
   "source": [
    "##### Evaluation of the models on AugLy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf034d-93e4-457d-8782-df9d0e6d06d2",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m spacy evaluate [path-to-model] [path-to-augly-data] --gpu-id X --output [output-path]\n",
    "```\n",
    "\n",
    "For example for BERT (encoder-only model):\n",
    "```bash\n",
    "python -m spacy evaluate ~/work/Code/WordCamouflage_Resiliance/code/output_models/Offen_SemEval_2019/bert-base-uncased_naive/model-best ~/work/Code/WordCamouflage_Resiliance/code/Spacy_Data/Offen_SemEval_2019/Leet_Data/AugLy/10_per/test.spacy --gpu-id 0 --output ~/work/Code/WordCamouflage_Resiliance/code/output_models/Offen_SemEval_2019/bert-base-uncased_naive/model-best/test_augly_10_result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9fad396-0daf-4ff4-aeee-d256f72b01ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet         #ChristineBlaseyFord is your #Kavanaugh accuser... #Liberals try this EVERY time... #ConfirmJudgeKavanaugh URL\n",
       "test_label                                                                                                               OFF\n",
       "Name: 30, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_offen.iloc[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "706bad69-31b3-4da8-ae34-10fce6d5350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_ReplaceSimilar = textaugs.ReplaceSimilarChars(p=1)\n",
    "aug_InsertPunctuation = textaugs.InsertPunctuationChars(granularity=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d2cc260-d3fe-403d-9624-b65649700ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H!E!l!l!o!o w.o.r.l.d']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_InsertPunctuation(\"HElloo world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78ac36ed-9ff2-4f15-92e1-230f81574364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>#SilsilaBadallteRishtonKa テag is filled with appreciation o₣ @ USER only.... Never watch£Đ this serial but I always wantⒺd appreciation for Adiťi... She iŜ underrated sᎥnce sta℟ⓣing of the shoŵ... She Ïs wỢrth Ớf aľl this appᖇeciatĬoŋ.. 👏 👏 👏</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>#ChristineBlaseyFord is your#K@vanau9h accuser...#L7|3erals t|2y +his EVERY tim3...#ConfirmJudgeKavanaugh URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>@ USER How is she hiding her ugly perso^ali+y. S|-|e is t|-|e worst.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>@USE:R om:g is: he :for :real: ?!?:!!??:????:) th:is h:appe:ned :in p:eru :like: 40 :year:s ag:o an:d th:e In:ti d:eval:uate:d so: fuc:king: muc:h th:at t:hey :had :to c:omol:etel:y ch:ange: the: coi:n sy:stem: bec:ause: our: mon:ey w:as w:orth:less: i-</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>#Halloween Sign If You Are Reάᚦing Thi₰#CreepingUp Behind ௶ou Ꮗood Wall ᗫeċôration#tmŤinstã#cwsigns URL via @ USER</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>#Antifa invaded a ጠemoriaȴ in recognition of Katę Steinle in AuŞtin, Texas, a₦d tried ṫo comfort hiᗰ it loỞked like when it wÀs struggling, but ℍas ℝeturnÊd to the Southern District Óf NÉw York, is r∑cuŝed froᛖ tℏe Michaҿł Cohen raid, Trump has my full support. Moving on.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>I'am sÔ sleepy.. {Boテ cüddle§ up to yoน} {#RoguẾBoŧ} \"</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>1) WoVV, saf3 sex! 7hat's hot! We lov3 a butch top who @lso receives! URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>#MeetTℍeŚpeaⓀeŗs 🙌 @ UՏER will ⓟreseӇt in όur event OIฟ 2018: Finpact-Global Impact through FinÅⓝcial Technologies. She is Senior Advisor GroŲp Sustainable ḞinaŇce and worked on green e₦ergy Ẵnd Ĉlimaté ri$k. Join us to mҾet Thina URL#oiw2018 URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>#WednesdayWisdom A^tifa calls +he right fascist when, in 4ll reality, th3y and the left are following the sa|V|e s&lt;enari() as the Thi12d Reich: indoctrination of []ur youth, tryi^g to control minorities and a total lack of understanding or kn[]wled6e of hi$toI2y.#W4lkAway</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                tweet  \\\n",
       "23                                 #SilsilaBadallteRishtonKa テag is filled with appreciation o₣ @ USER only.... Never watch£Đ this serial but I always wantⒺd appreciation for Adiťi... She iŜ underrated sᎥnce sta℟ⓣing of the shoŵ... She Ïs wỢrth Ớf aľl this appᖇeciatĬoŋ.. 👏 👏 👏   \n",
       "30                                                                                                                                                                      #ChristineBlaseyFord is your#K@vanau9h accuser...#L7|3erals t|2y +his EVERY tim3...#ConfirmJudgeKavanaugh URL   \n",
       "39                                                                                                                                                                                                               @ USER How is she hiding her ugly perso^ali+y. S|-|e is t|-|e worst.   \n",
       "63                      @USE:R om:g is: he :for :real: ?!?:!!??:????:) th:is h:appe:ned :in p:eru :like: 40 :year:s ag:o an:d th:e In:ti d:eval:uate:d so: fuc:king: muc:h th:at t:hey :had :to c:omol:etel:y ch:ange: the: coi:n sy:stem: bec:ause: our: mon:ey w:as w:orth:less: i-   \n",
       "65                                                                                                                                                                 #Halloween Sign If You Are Reάᚦing Thi₰#CreepingUp Behind ௶ou Ꮗood Wall ᗫeċôration#tmŤinstã#cwsigns URL via @ USER   \n",
       "..                                                                                                                                                                                                                                                                                ...   \n",
       "836  #Antifa invaded a ጠemoriaȴ in recognition of Katę Steinle in AuŞtin, Texas, a₦d tried ṫo comfort hiᗰ it loỞked like when it wÀs struggling, but ℍas ℝeturnÊd to the Southern District Óf NÉw York, is r∑cuŝed froᛖ tℏe Michaҿł Cohen raid, Trump has my full support. Moving on.   \n",
       "846                                                                                                                                                                                                                            I'am sÔ sleepy.. {Boテ cüddle§ up to yoน} {#RoguẾBoŧ} \"   \n",
       "850                                                                                                                                                                                                         1) WoVV, saf3 sex! 7hat's hot! We lov3 a butch top who @lso receives! URL   \n",
       "856                            #MeetTℍeŚpeaⓀeŗs 🙌 @ UՏER will ⓟreseӇt in όur event OIฟ 2018: Finpact-Global Impact through FinÅⓝcial Technologies. She is Senior Advisor GroŲp Sustainable ḞinaŇce and worked on green e₦ergy Ẵnd Ĉlimaté ri$k. Join us to mҾet Thina URL#oiw2018 URL   \n",
       "858  #WednesdayWisdom A^tifa calls +he right fascist when, in 4ll reality, th3y and the left are following the sa|V|e s<enari() as the Thi12d Reich: indoctrination of []ur youth, tryi^g to control minorities and a total lack of understanding or kn[]wled6e of hi$toI2y.#W4lkAway   \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "23         NOT         True  \n",
       "30         OFF         True  \n",
       "39         OFF         True  \n",
       "63         OFF         True  \n",
       "65         NOT         True  \n",
       "..         ...          ...  \n",
       "836        NOT         True  \n",
       "846        NOT         True  \n",
       "850        NOT         True  \n",
       "856        NOT         True  \n",
       "858        NOT         True  \n",
       "\n",
       "[86 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_test_offen_10_per[df_test_offen_10_per[\"Camouflaged\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b997c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2de690-0082-449e-8f17-d65dbc51da2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Table Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607930b5",
   "metadata": {},
   "source": [
    "Random examples extracted from the camouflaged data across different levels of complexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d353a431-481d-44f1-ab80-8fc32eb722f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 111.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 70.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 62.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 15/15 [00:00<00:00, 1848.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in 'tweet' column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camouflaged</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet  test_label\n",
       "Camouflaged                   \n",
       "True         15     15        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL                                                                                             \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  \n",
       "286  NOT        \n",
       "169  NOT        \n",
       "363  NOT        \n",
       "258  OFF        \n",
       "74   NOT        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 1 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USÆR @ųSER @ūSȇR Gun Control is nothing but pŒƟpl₤ CØntrØl. The left wants Gun CΘntrʘl ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North KȱrƏa.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DįSARMTHꬲM #ȃNTĩF∆  Givę Jăck here a participation trophy! ŬRL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTįFA ATTACKED CʘPS, ăRRƸST3D AT RALLY IN DēNVėR, MEDĩ∀ BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CĭncĭnnatĭƸnqƱĭrƸr steps into #G_n Cöntröl F¡NALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ŬRL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@ƱSƐR Well she is ChƱck SchƱmer's cousin or niece or something so she is obviously part of the DNC conspiracy against NĭxΘn. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @USÆR @ųSER @ūSȇR Gun Control is nothing but pŒƟpl₤ CØntrØl. The left wants Gun CΘntrʘl ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North KȱrƏa.   \n",
       "169  #DįSARMTHꬲM #ȃNTĩF∆  Givę Jăck here a participation trophy! ŬRL                                                                                                                                                                                              \n",
       "363  6 ANTįFA ATTACKED CʘPS, ăRRƸST3D AT RALLY IN DēNVėR, MEDĩ∀ BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #CĭncĭnnatĭƸnqƱĭrƸr steps into #G_n Cöntröl F¡NALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ŬRL                                                                                             \n",
       "74   @ƱSƐR Well she is ChƱck SchƱmer's cousin or niece or something so she is obviously part of the DNC conspiracy against NĭxΘn. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 2 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@'ū'S'E'R @(_)ŝEŗ @UşEř ĝū₦ Cꬽn†`r0∟ is nothing but )|)&gt;)e)ŏ)|)&gt;)∟)e /C/o/ͷ/₸/r/o/ļ. The left wants Gũñ CØntЯʘ| ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North |&lt;oЯeă.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#D1śɅŗ]V[₸HE]V[ #A|\\|TıFA  Gıv℮ ]ą©k here a participation trophy! \"U\"®\"L</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 äNţI&lt;Fä ATTACKED ĉOP$, |4|ŗ|ŗ|E|ϟ|7|E|D AT RALLY IN DEηṿE#R, MED!A BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#Cin/cin\\naŧi;En^quiℝeℝ steps into #ģũ{\\} =C=ŏ=ŋ=t=ŗ=ŏ=ℓ FINäłłꭚ. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. UR∟</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@U.5.ƹR Well she is }č}h}Ʊ}č}k ś©ĥŬmer's cousin or niece or something so she is obviously part of the Dŉ[ conspiracy against !N!ĭ!ꭙ!o!n. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                   tweet  \\\n",
       "286  @'ū'S'E'R @(_)ŝEŗ @UşEř ĝū₦ Cꬽn†`r0∟ is nothing but )|)>)e)ŏ)|)>)∟)e /C/o/ͷ/₸/r/o/ļ. The left wants Gũñ CØntЯʘ| ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North |<oЯeă.   \n",
       "169  #D1śɅŗ]V[₸HE]V[ #A|\\|TıFA  Gıv℮ ]ą©k here a participation trophy! \"U\"®\"L                                                                                                                                                                                                              \n",
       "363  6 äNţI<Fä ATTACKED ĉOP$, |4|ŗ|ŗ|E|ϟ|7|E|D AT RALLY IN DEηṿE#R, MED!A BLACKOUT  URL                                                                                                                                                                                                    \n",
       "258  #Cin/cin\\naŧi;En^quiℝeℝ steps into #ģũ{\\} =C=ŏ=ŋ=t=ŗ=ŏ=ℓ FINäłłꭚ. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. UR∟                                                                                                         \n",
       "74   @U.5.ƹR Well she is }č}h}Ʊ}č}k ś©ĥŬmer's cousin or niece or something so she is obviously part of the Dŉ[ conspiracy against !N!ĭ!ꭙ!o!n. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                            \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 3 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@UϟEℝ @µSœR @]ų]S]E]Ɽ Gun ℂöπţröĺ is nothing but +p+ë+ꬽ+p+ℒ+℮ ç[]₦ţ®Ɵℒ. The left wants geeบņ C&lt;&gt;ทₜr&lt;&gt;l ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Kȱrea.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#[)ISA/2₥₸HE₥ #*A*N*T*I*|*#*A  Gĩve Jaç|&lt; here a participation trophy! ŬRL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6  TI FAAN ATTACKED CōPS, +A+R+R+E+S+†+E+[+) AT RALLY IN |)Eℕ|/Eŕ, MED|Â BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#+ℂ+ï+ŉ+ℂ+ï+ŉ+ŉ+a+'+]+[+'+ï+E+ŉ+q+u+ï+𝔑+e+𝔑 steps into #Gun {ŏn}trʘl FINÃ|_|Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ũRL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@.ʊ.∫.Æ.R Well she is Chücĸ \\$\\c\\h\\u\\ℳ\\ĕ\\r's cousin or niece or something so she is obviously part of the ďN© conspiracy against /\\/įχȱ/\\/. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                         tweet  \\\n",
       "286  @UϟEℝ @µSœR @]ų]S]E]Ɽ Gun ℂöπţröĺ is nothing but +p+ë+ꬽ+p+ℒ+℮ ç[]₦ţ®Ɵℒ. The left wants geeบņ C<>ทₜr<>l ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Kȱrea.   \n",
       "169  #[)ISA/2₥₸HE₥ #*A*N*T*I*|*#*A  Gĩve Jaç|< here a participation trophy! ŬRL                                                                                                                                                                                                  \n",
       "363  6  TI FAAN ATTACKED CōPS, +A+R+R+E+S+†+E+[+) AT RALLY IN |)Eℕ|/Eŕ, MED|Â BLACKOUT  URL                                                                                                                                                                                      \n",
       "258  #+ℂ+ï+ŉ+ℂ+ï+ŉ+ŉ+a+'+]+[+'+ï+E+ŉ+q+u+ï+𝔑+e+𝔑 steps into #Gun {ŏn}trʘl FINÃ|_|Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. ũRL                                                                                  \n",
       "74   @.ʊ.∫.Æ.R Well she is Chücĸ \\$\\c\\h\\u\\ℳ\\ĕ\\r's cousin or niece or something so she is obviously part of the ďN© conspiracy against /\\/įχȱ/\\/. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- AugLy --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USE;R @U;SER ;@USE;R Gu;n Co;ntro;l is; not;hing; but; peo;ple ;cont;rol.; The; lef;t wa;nts ;gun ;cont;rol ;ONLY; bec;ause; the;y do; NOT; wan;t us; lit;tle ;guys; arm;ed &amp;;amp;; abl;e to; def;end ;ours;elve;s. M;akes; it ;hard;er t;o ge;t us; beh;ind ;barb;ed-w;ire ;fenc;es a;s th;ey s;aw i;n No;rth ;Kore;a.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#Cίncinⓝa⍡iℯnqϋirer steps into#Gun Control FINALLY. Took a#MassShhoting on their damn door step foᶉ Ṫheir#editors tỞ wakҿ ửp, ხut better Ļate than Ŋever. URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                        tweet  \\\n",
       "286  @USE;R @U;SER ;@USE;R Gu;n Co;ntro;l is; not;hing; but; peo;ple ;cont;rol.; The; lef;t wa;nts ;gun ;cont;rol ;ONLY; bec;ause; the;y do; NOT; wan;t us; lit;tle ;guys; arm;ed &;amp;; abl;e to; def;end ;ours;elve;s. M;akes; it ;hard;er t;o ge;t us; beh;ind ;barb;ed-w;ire ;fenc;es a;s th;ey s;aw i;n No;rth ;Kore;a.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                                                                                            \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                                                                                   \n",
       "258  #Cίncinⓝa⍡iℯnqϋirer steps into#Gun Control FINALLY. Took a#MassShhoting on their damn door step foᶉ Ṫheir#editors tỞ wakҿ ửp, ხut better Ļate than Ŋever. URL                                                                                                                                                              \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                                                                             \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 10% Leet ####\n",
    "toy_test = df_test_offen.sample(15, random_state=2)\n",
    "\n",
    "# resiliance_level = \"Level_3\"\n",
    "# resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "\n",
    "# augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "#         extractor_type=\"yake\",\n",
    "#         max_top_n=20,\n",
    "#         leet_punt_prb=0.9,\n",
    "#         leet_change_prb=0.5,\n",
    "#         leet_change_frq=0.8,\n",
    "#         leet_uniform_change=0.6,\n",
    "#         punt_hyphenate_prb=0.7,\n",
    "#         punt_uniform_change_prb=0.95,\n",
    "#         punt_word_splitting_prb=0.8,\n",
    "#         method=resiliance_intermediate,\n",
    "# )\n",
    "\n",
    "resiliance_level = \"Level_1.1\"\n",
    "resiliance_easy = [\"basic_leetspeak\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.8,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.5,\n",
    "        method=resiliance_easy,\n",
    ")\n",
    "\n",
    "# Create a test dataframe with 10% of leeted tweets\n",
    "df_level_1 = create_leet_augmenter_df(\n",
    "    df_ori=toy_test, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "resiliance_level = \"Level_2.1\"\n",
    "resiliance_intermediate = [\"intermediate_leetspeak\", \"punct_camo\"]\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.9,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        method=resiliance_intermediate,\n",
    ")\n",
    "\n",
    "df_level_2 = create_leet_augmenter_df(\n",
    "    df_ori=toy_test, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "resiliance_level = \"Level_3.1\"\n",
    "resiliance_advanced = [\"advanced_leetspeak\", \"punct_camo\", \"inv_camo\"]\n",
    "\n",
    "augmenter = WordCamouflage_Augmenter.augmenter(\n",
    "        extractor_type=\"yake\",\n",
    "        max_top_n=5,\n",
    "        leet_punt_prb=0.4,\n",
    "        leet_change_prb=0.5,\n",
    "        leet_change_frq=0.8,\n",
    "        leet_uniform_change=0.6,\n",
    "        punt_hyphenate_prb=0.7,\n",
    "        punt_uniform_change_prb=0.95,\n",
    "        punt_word_splitting_prb=0.8,\n",
    "        inv_max_dist=4,\n",
    "        inv_only_max_dist_prb=0.5,\n",
    "        method=resiliance_advanced,\n",
    ")\n",
    "\n",
    "df_level_3 = create_leet_augmenter_df(\n",
    "    df_ori=toy_test, frac=1, augmenter=augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "\n",
    "resiliance_level = \"AugLy\"\n",
    "\n",
    "########### 10% ############\n",
    "df_augly = create_augly_augmentation(\n",
    "    df_ori=toy_test, frac=1, augmenter=random_augly_augmenter, column_to_leet = \"tweet\"\n",
    ")\n",
    "# toy_test = df_test_offen.sample(15, random_state=42)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "display(toy_test.head())\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 1 --------------\")\n",
    "display(df_level_1.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 2 --------------\")\n",
    "display(df_level_2.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 3 --------------\")\n",
    "display(df_level_3.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- AugLy --------------\")\n",
    "display(df_augly.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5851a9-2dd0-4397-ac69-42e1e6a29237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL                                                                                             \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  \n",
       "286  NOT        \n",
       "169  NOT        \n",
       "363  NOT        \n",
       "258  OFF        \n",
       "74   NOT        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_test = df_test_offen.sample(15, random_state=2)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "display(toy_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c57d2b21-a828-419f-97ff-efe28cfa93d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @USER @USER @USER Gun Control is nothing but people control. The left wants gun control ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Korea.   \n",
       "169  #DISARMTHEM #ANTIFA  Give Jack here a participation trophy! URL                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                     \n",
       "258  #CincinnatiEnquirer steps into #Gun Control FINALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URL                                                                                             \n",
       "74   @USER Well she is Chuck Schumer's cousin or niece or something so she is obviously part of the DNC conspiracy against Nixon. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.               \n",
       "\n",
       "    test_label  \n",
       "286  NOT        \n",
       "169  NOT        \n",
       "363  NOT        \n",
       "258  OFF        \n",
       "74   NOT        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 1 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@ƱS₤R @ųSER @ųSER G_n Cőntről is nothing but peØple Control. The lëft wants Gun Cōntrōl ONLY because they do NOT want us little gũys αrm₤d &amp;amp; able to d3f3nd ourselves. Mäkës it hąrd3r to get us behind bȁrbed-wíre fꬲncės as they saw in N0rth KōrƏą.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISăRMTHēM #äNTĭFä  Gĩvę Jäck here a p∆rtïc!p∆t!0n trŏphy! ŬRL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANT1FA ∀TT∀CKED CƟPS, ARRESTED AT RДLLY IN DƏNV3R, M€DI∀ BL@CKʘบT  ũRL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#C1nc1nnαt1ěnqบ1rěr stƸps into #Gün C&lt;&gt;ntr&lt;&gt;l FíNALLY. Took a #MДssShhƟtįng on their dɅmn dŏŏr stƐp for their #Œdįt&lt;&gt;rs to wākƏ up, but better lȁte than never. ŬRL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@ųSER Well she is Chųck Sch_mėr's cʘũs1n or nįÆcē or something so she is obviously pⱥrt of the DNC cʘnsp!r4cy against Nixon. Or part of the D€€p St∆tŒ. Or a Z¡&lt;&gt;n¡st plöt. Or 'Big c&lt;&gt;mÆdy'. Or whatever the hƐll tȱdⱥy's cȱnspírȃcy thƏőry is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          tweet  \\\n",
       "286  @ƱS₤R @ųSER @ųSER G_n Cőntről is nothing but peØple Control. The lëft wants Gun Cōntrōl ONLY because they do NOT want us little gũys αrm₤d &amp; able to d3f3nd ourselves. Mäkës it hąrd3r to get us behind bȁrbed-wíre fꬲncės as they saw in N0rth KōrƏą.   \n",
       "169  #DISăRMTHēM #äNTĭFä  Gĩvę Jäck here a p∆rtïc!p∆t!0n trŏphy! ŬRL                                                                                                                                                                                              \n",
       "363  6 ANT1FA ∀TT∀CKED CƟPS, ARRESTED AT RДLLY IN DƏNV3R, M€DI∀ BL@CKʘบT  ũRL                                                                                                                                                                                     \n",
       "258  #C1nc1nnαt1ěnqบ1rěr stƸps into #Gün C<>ntr<>l FíNALLY. Took a #MДssShhƟtįng on their dɅmn dŏŏr stƐp for their #Œdįt<>rs to wākƏ up, but better lȁte than never. ŬRL                                                                                          \n",
       "74   @ųSER Well she is Chųck Sch_mėr's cʘũs1n or nįÆcē or something so she is obviously pⱥrt of the DNC cʘnsp!r4cy against Nixon. Or part of the D€€p St∆tŒ. Or a Z¡<>n¡st plöt. Or 'Big c<>mÆdy'. Or whatever the hƐll tȱdⱥy's cȱnspírȃcy thƏőry is.             \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 2 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@#U#ş#ȇ#R @USĕ® @UśƹR ğ(_)|\\| çoņႵrol is nothing but pe&lt;&gt;ple ,ć,ö,|,\\,|,t,ŗ,ö,l. The left wants G(_)η çΘnₜЯʘ| ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Koreȁ.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DI5ARMᵗHœM #4ℕǂ1ϝ4  $ġ$ı$v$e Jac𝕂 here a participation trophy! URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 :A:N:ͳ:I:ſ:A ATTACKED CØℙ§, äR`REš†EĎ AT RALLY IN |)Eň۷ER, M℮|)įA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#[¡n[¡n-năŧ¡En-qų¡ŗeŗ steps into #G(_)n ćonŢŗol F1ℕALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URł</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@Ŭ$EЯ Well she is Cĥucķ #S#[#h#ų#m#e#r's cousin or niece or something so she is obviously part of the DNC conspiracy against Ni&gt;&lt;on. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                tweet  \\\n",
       "286  @#U#ş#ȇ#R @USĕ® @UśƹR ğ(_)|\\| çoņႵrol is nothing but pe<>ple ,ć,ö,|,\\,|,t,ŗ,ö,l. The left wants G(_)η çΘnₜЯʘ| ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North Koreȁ.   \n",
       "169  #DI5ARMᵗHœM #4ℕǂ1ϝ4  $ġ$ı$v$e Jac𝕂 here a participation trophy! URL                                                                                                                                                                                                                \n",
       "363  6 :A:N:ͳ:I:ſ:A ATTACKED CØℙ§, äR`REš†EĎ AT RALLY IN |)Eň۷ER, M℮|)įA BLACKOUT  URL                                                                                                                                                                                                  \n",
       "258  #[¡n[¡n-năŧ¡En-qų¡ŗeŗ steps into #G(_)n ćonŢŗol F1ℕALLY. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. URł                                                                                                               \n",
       "74   @Ŭ$EЯ Well she is Cĥucķ #S#[#h#ų#m#e#r's cousin or niece or something so she is obviously part of the DNC conspiracy against Ni><on. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                             \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- LEVEL 3 --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@USE|` @{└{┘{S{E{R @ũ$E2 \"G\"u\"|\"\\\"| \"tŕo1Con is nothing but _peo_p_le /C/ŏ/n/t/r/ꬽ/|. The left wants {,un @C@[@]@n@t@r@ö@ļ ONLY because they do NOT want us little guys armed &amp;amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North 1&lt;orƐa.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#ĐıSÃRɱTⱨE₥ #∀N]TI]F∀  (+į√e Jac/&lt; here a participation trophy! &amp;ū&amp;R&amp;L</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ∆NŦI|F∆ ATTACKED COPş, AR^RēSȶēD AT RALLY IN DEη۷ER, DIA^^ě_ BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#©||\\|©||\\||\\|āt|E|\\|qu|ℝeℝ steps into #~G~ʊ~[~\\~] !ċ!o!n!Ⴕ!ŕ!o!£ }₣}ĭ}{}\\}}}A}L}L}Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. üRł</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@ŭ$ER Well she is ↄhu&lt;Ķ šↄhu^^ȇr's cousin or niece or something so she is obviously part of the \\D\\N\\C conspiracy against  ń i χ φ ń. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                              tweet  \\\n",
       "286  @USE|` @{└{┘{S{E{R @ũ$E2 \"G\"u\"|\"\\\"| \"tŕo1Con is nothing but _peo_p_le /C/ŏ/n/t/r/ꬽ/|. The left wants {,un @C@[@]@n@t@r@ö@ļ ONLY because they do NOT want us little guys armed &amp; able to defend ourselves. Makes it harder to get us behind barbed-wire fences as they saw in North 1<orƐa.   \n",
       "169  #ĐıSÃRɱTⱨE₥ #∀N]TI]F∀  (+į√e Jac/< here a participation trophy! &ū&R&L                                                                                                                                                                                                                           \n",
       "363  6 ∆NŦI|F∆ ATTACKED COPş, AR^RēSȶēD AT RALLY IN DEη۷ER, DIA^^ě_ BLACKOUT  URL                                                                                                                                                                                                                     \n",
       "258  #©||\\|©||\\||\\|āt|E|\\|qu|ℝeℝ steps into #~G~ʊ~[~\\~] !ċ!o!n!Ⴕ!ŕ!o!£ }₣}ĭ}{}\\}}}A}L}L}Y. Took a #MassShhoting on their damn door step for their #editors to wake up, but better late than never. üRł                                                                                                \n",
       "74   @ŭ$ER Well she is ↄhu<Ķ šↄhu^^ȇr's cousin or niece or something so she is obviously part of the \\D\\N\\C conspiracy against  ń i χ φ ń. Or part of the Deep State. Or a Zionist plot. Or 'Big comedy'. Or whatever the hell today's conspiracy theory is.                                          \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------- AugLy --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>test_label</th>\n",
       "      <th>Camouflaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>@ US3R @ USER @ USER Gun Control is ^oth|ng but p3ople control. T/-/e left wants gun control O^LY because they dD ^OT want us little 9uys armed &amp; amp; able to |)efend ()urse|_ves. Makes it harder to get us behind barbed-wire fences as th3y saw i^ North Kor3a.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>#DISARMTHEM#ANTIF4 Give Jack her3 a participation trophy! U/2L</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>#Cin!cinn!atiE!nqui!rer !step!s in!to #!Gun !Cont!rol !FINA!LLY.! Too!k a !#Mas!sShh!otin!g on! the!ir d!amn !door! ste!p fo!r th!eir !#edi!tors! to !wake! up,! but! bet!ter !late! tha!n ne!ver.! URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>@USE.R We.ll s.he i.s Ch.uck .Schu.mer'.s co.usin. or .niec.e or. som.ethi.ng s.o sh.e is. obv.ious.ly p.art .of t.he D.NC c.onsp.irac.y ag.ains.t Ni.xon.. Or .part. of .the .Deep. Sta.te. .Or a. Zio.nist. plo.t. O.r 'B.ig c.omed.y'. .Or w.hate.ver .the .hell. tod.ay's. con.spir.acy .theo.ry i.s.</td>\n",
       "      <td>NOT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         tweet  \\\n",
       "286  @ US3R @ USER @ USER Gun Control is ^oth|ng but p3ople control. T/-/e left wants gun control O^LY because they dD ^OT want us little 9uys armed & amp; able to |)efend ()urse|_ves. Makes it harder to get us behind barbed-wire fences as th3y saw i^ North Kor3a.                                         \n",
       "169  #DISARMTHEM#ANTIF4 Give Jack her3 a participation trophy! U/2L                                                                                                                                                                                                                                              \n",
       "363  6 ANTIFA ATTACKED COPS, ARRESTED AT RALLY IN DENVER, MEDIA BLACKOUT  URL                                                                                                                                                                                                                                    \n",
       "258  #Cin!cinn!atiE!nqui!rer !step!s in!to #!Gun !Cont!rol !FINA!LLY.! Too!k a !#Mas!sShh!otin!g on! the!ir d!amn !door! ste!p fo!r th!eir !#edi!tors! to !wake! up,! but! bet!ter !late! tha!n ne!ver.! URL                                                                                                     \n",
       "74   @USE.R We.ll s.he i.s Ch.uck .Schu.mer'.s co.usin. or .niec.e or. som.ethi.ng s.o sh.e is. obv.ious.ly p.art .of t.he D.NC c.onsp.irac.y ag.ains.t Ni.xon.. Or .part. of .the .Deep. Sta.te. .Or a. Zio.nist. plo.t. O.r 'B.ig c.omed.y'. .Or w.hate.ver .the .hell. tod.ay's. con.spir.acy .theo.ry i.s.   \n",
       "\n",
       "    test_label  Camouflaged  \n",
       "286  NOT        True         \n",
       "169  NOT        True         \n",
       "363  NOT        True         \n",
       "258  OFF        True         \n",
       "74   NOT        True         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# toy_test = df_test_offen.sample(15, random_state=42)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "display(toy_test.head())\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 1 --------------\")\n",
    "display(df_level_1.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 2 --------------\")\n",
    "display(df_level_2.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- LEVEL 3 --------------\")\n",
    "display(df_level_3.head(5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------------- AugLy --------------\")\n",
    "display(df_augly.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7552c5-028c-44a3-877c-aec577050d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NAIVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83718906-17e1-464d-91c2-36f75164cc77",
   "metadata": {},
   "source": [
    "- [x] Naive\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best ~/work/Code/WordCamouflage_Resiliance/code/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best/test_result.json\n",
    "    \n",
    "python -m spacy evaluate ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best ~/work/Code/WordCamouflage_Resiliance/code/Spacy_Data/Constraint/Leet_Data/Level_3.2/100_per/test.spacy  --gpu-id 1 --output ~/work/Code/WordCamouflage_Resiliance/code/output_models/Constraint/bert-base-uncased_constraint-naive/model-best/test_100_level_3.2_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    \n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    \n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_naive/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c5094-4476-4f68-842b-e3e403ce8bb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 10_leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c64ac1-604b-425d-b2ba-a5ed1364dfd1",
   "metadata": {},
   "source": [
    "- [x] 10_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>    \n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "       ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd226c82-c11c-47b7-aaae-442f1d4d40e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 25 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8865f2-44d4-4434-a344-2c1a1c358bc3",
   "metadata": {},
   "source": [
    "- [ ] 25_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52828a32-846b-4459-a2ff-5bb1064a825c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 50 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be0994-0f6a-4b0d-9cb4-7d94f8213f3d",
   "metadata": {},
   "source": [
    "- [ ] 50_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf1e0d-6bdb-4396-a40a-cac4d8f80d31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 75 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714d0cc-0e99-4764-bdca-56e1de1e942d",
   "metadata": {},
   "source": [
    "- [ ] 75_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d925a87-a59a-4779-afcc-376ff3b34d0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 100 leet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03aea9-d645-4e9c-bf34-d7ddb287031b",
   "metadata": {},
   "source": [
    "- [ ] 100_leet\n",
    "    - [x] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [x] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_leet/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b5929-aa6a-49ec-977e-0251d31f0dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 10_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a89ba-74b5-45fd-a7b8-16f0daf30003",
   "metadata": {},
   "source": [
    "- [ ] 10_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_10_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927942aa-c8d8-4db9-975a-483545ec01bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 25_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7ebe0-777c-4835-9e3d-0c5c9b44894a",
   "metadata": {},
   "source": [
    "- [ ] 25_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_25_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00868dd8-7f80-4ba7-aef4-0c0df98aede6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 50_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e3951-63b0-4717-9f05-5121049dd6d7",
   "metadata": {},
   "source": [
    "- [ ] 50_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_50_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382675d8-840e-4041-94a5-132c547eca33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 75_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2694e-854a-4118-9fb5-1804bfa6d94d",
   "metadata": {},
   "source": [
    "- [ ] 75_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_75_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589474f7-168a-41e9-8ccf-a59f75a9b959",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 100_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b59a80-7d50-45c6-a9c8-1a4d581f4b41",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- [ ] 100_augmented\n",
    "    - [ ] test\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Ori_Data/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 10\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/10_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_10_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 25\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/25_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_25_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 50\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/50_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_50_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 75\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/75_per/test.spacy  --gpu-id 0 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_75_leet_result.json\n",
    "    ```\n",
    "    <br>\n",
    "    - [ ] 100\n",
    "    ```bash\n",
    "    python -m spacy evaluate /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best /home/alvaro/data/WordCamouflage_Resiliance/Spacy_Data/Constraint/Leet_Data/100_per/test.spacy  --gpu-id 1 --output /home/alvaro/data/WordCamouflage_Resiliance/output_models/Constraint/bert-base-uncased_Constraint_100_augmented/model-best/test_100_leet_result.json\n",
    "    ```\n",
    "    <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CamouflageKernel",
   "language": "python",
   "name": "camouflagekernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ac8b2602792a0e1037910f4d8758f49024557e1c1607b4d9be870b3f2a26f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
